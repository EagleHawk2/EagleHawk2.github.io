---
title: Hadoop
date: 2023-06-29 19:18:30
tags:
categories: å¤§æ•°æ®é›†ç¾¤æ­å»º
---



**å†™åœ¨æœ€å‰ï¼Œæœ¬æ–‡ä¸­æ‰€ä½¿ç”¨çš„é…ç½®æ˜¯å¯ä»¥è¾¾åˆ°ä½¿ç”¨hadoopé›†ç¾¤æ­å»ºçš„æœ€åŸºæœ¬é…ç½®ï¼Œå¦‚æœæœ‰å…¶ä»–éœ€æ±‚ï¼ŒæŒ‰ç…§è‡ªå·±çš„éœ€æ±‚å»å¢åŠ æˆ–åˆ é™¤é…ç½®æ–‡ä»¶å³å¯ï¼Œæœ¬æ–‡åªé˜è¿°æœ€åŸºæœ¬ç”¨ä¾‹ï¼ŒåæœŸä¼˜åŒ–å°±é è‡ªå·±å•¦~åŠ æ²¹å™¢ğŸ’ª**



## Hadoopå®Œå…¨åˆ†å¸ƒå¼å®‰è£…

**è§£å‹åˆ°æŒ‡å®šæ–‡ä»¶å¤¹**

```
tar -zxvf /opt/software/hadoop.tar.gz -C /opt/module/
```

æ¯ä¸€ä¸ªæ–‡ä»¶çš„é…ç½®æ— å¤–ä¹å°±ä¸¤ä¸ªé…ç½®å†…å®¹ï¼Œä¸€ä¸ªæ˜¯ç¯å¢ƒå˜é‡çš„é…ç½®ï¼Œä¸€ä¸ªæ˜¯æ–‡ä»¶å†…çš„é…ç½®ã€‚è€Œè¦ä½¿ç”¨çš„ç¬¬ä¸€æ­¥ï¼Œè‡ªç„¶å°±æ˜¯å®‰è£…ã€‚æˆ‘ä»¬é‡‡ç”¨çš„å®‰è£…æ–¹æ³•æ˜¯å‹ç¼©åŒ…å®‰è£…ï¼Œæ‰€ä»¥æˆ‘ä»¬å®‰è£…çš„ç¬¬ä¸€æ­¥ï¼Œæ˜¯è§£å‹æ–‡ä»¶

------

### **@é…ç½®ç¯å¢ƒå˜é‡**

```
vi /etc/profile
```

```
export HADOOP_HOME=/opt/module/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

ä»¥ä¸Šæ˜¯hadoop2.7ç‰ˆæœ¬ä¸­ç¯å¢ƒå˜é‡éœ€è¦é…ç½®çš„å†…å®¹ã€‚

åœ¨è¿™é‡Œå†æä¸€ä¸ªLinux shellè¯­æ³•ï¼Œå¯ä»¥å¿«é€ŸæŸ¥è¯¢è·¯å¾„ï¼š

**æŸ¥è¯¢å½“å‰æ‰€åœ¨è·¯å¾„å**

```
pwd
```

ä½†æ˜¯åœ¨hadoopçš„3.1.3çš„ç‰ˆæœ¬ä¸­ï¼Œéœ€è¦åŠ å…¥ä»¥ä¸‹å†…å®¹ï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚å¹¸è¿çš„æ˜¯ï¼ŒæŠ¥é”™å†…å®¹ä¹Ÿä¼šæç¤ºä½ å¦‚ä½•å»ä¿®æ”¹ç¯å¢ƒå˜é‡ã€‚ï¼ˆè¿™æ˜¯ç¬¬ä¸€ç§é…ç½®æ–¹å¼ï¼Œæ–‡ç« ç»“å°¾çš„è¡¥å……è¯´æ˜ä¸­ï¼Œæˆ‘ä¼šè®²è¿°ç¬¬äºŒç§é…ç½®æ–¹å¼ğŸ¤£å½“ç„¶æˆ‘è¿˜æ˜¯è§‰å¾—è¿™ç§é…ç½®èµ·æ¥æ›´æ–¹ä¾¿å•¦ï¼‰

```
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```

hadoop3.1.3ç‰ˆæœ¬éœ€è¦åŠ å…¥ä»¥ä¸Šå†…å®¹ã€‚

**å…¨å±€ç¯å¢ƒå˜é‡ç”Ÿæ•ˆ**

```
source /etc/profile
```

æ¯æ¬¡ä¿®æ”¹å®Œç¯å¢ƒå˜é‡éœ€è¦å¯¹ç¯å¢ƒå˜é‡è¿›è¡Œç”Ÿæ•ˆå‘½ä»¤ã€‚

å…³äºç¯å¢ƒå˜é‡çš„ç”Ÿæ•ˆï¼Œå†è¡¥å……ä¸€äº›å†…å®¹ã€‚

å¦‚æœå½“å‡ºç°å…¨å±€å˜é‡é…ç½®å‡ºç°é—®é¢˜å¯¼è‡´åŸºç¡€å‘½ä»¤å¤±æ•ˆçš„æƒ…å†µâ€”â€”

æœ‰ä¸€ä¸ªæœ€é«˜ä½çš„å‘½ä»¤å¯ä»¥ä½¿ç”¨ï¼š

**å…¨å±€å˜é‡ä¿®æ”¹**

```
/bin/vi /etc/profile
```

**å…¨å±€å˜é‡ç”Ÿæ•ˆ**

```
./etc/profile
```

### **@é…ç½®æ–‡ä»¶**

**é…ç½®æ–‡ä»¶æ¨¡æ¿**

```
<configuration>
	<property>
		<name></name>
		<value></value>
	</property>
</configuration>
```

æ²¡å•¥ç”¨ï¼Œå­˜ä¸€ä¸ªï¼Œæ–¹ä¾¿å¤åˆ¶ç²˜è´´ã€‚

------

#### #core-site.xml

```
vi /opt/module/hadoop/etc/hadoop/core-site.xml
```

```
<configuration>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://master:9000</value>
	</property>
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/opt/module/hadoop/data</value>
	</property>
</configuration>
```

NameNodeå†…éƒ¨é€šä¿¡çš„ç«¯å£åœ¨hadoop3.1.3ç‰ˆæœ¬ä¸­æœ‰ä¸‰ä¸ªå¸¸ç”¨ç«¯å£å·ï¼š8020ï¼Œ9000ï¼Œ9820ï¼Œæˆ‘å¸¸ç”¨çš„ç«¯å£å·æ˜¯9000ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒClickhouseçš„é»˜è®¤ç«¯å£å·ä¹Ÿæ˜¯9000ï¼Œæ‰€ä»¥å½“éœ€è¦é…ç½®ä¸¤ä¸ªçš„æ—¶å€™ï¼Œæœ‰ä¸€ä¸ªéœ€è¦æ›´æ”¹ç«¯å£å·ï¼Œå¦åˆ™ä¼šå‡ºç°å ç”¨çš„æƒ…å†µã€‚

hadoop.tmp.dirå­˜å‚¨çš„æ˜¯hadoopæ–‡ä»¶ç³»ç»Ÿä¾èµ–ï¼Œå®ƒæœ‰é»˜è®¤ä½ç½®åœ¨æ ¹ç›®å½•çš„/tmpä¸‹ï¼Œä½†æ˜¯å­˜å‚¨äºé»˜è®¤ä½ç½®ä¸‹çš„tmpæ–‡ä»¶æ˜¯ä¸´æ—¶æ–‡ä»¶ï¼Œåœ¨æœºå™¨è¿›è¡Œå¼€å…³æ—¶ä¼šé‡ç½®ä¸¢å¤±æ–‡ä»¶ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨ä¸ºå®ƒé…ç½®ä¸€ä¸ªè·¯å¾„ã€‚æˆ‘å†™çš„dataæ–‡ä»¶å¤¹æ˜¯å®‰è£…åŒ…ä¸è‡ªå¸¦çš„ï¼Œdataæ–‡ä»¶åœ¨æ‰§è¡ŒNameNodeçš„åˆå§‹åŒ–æ—¶ä¼šè‡ªåŠ¨ç”Ÿæˆï¼Œä¹Ÿå¯ä»¥è‡ªå·±ä¸ºå®ƒåˆ›å»ºã€‚

**æ–°å»ºdataæ–‡ä»¶å¤¹**

```
mkdir data
```

------

#### #hdfs-site.xml

```
vi /opt/module/hadoop/etc/hadoop/hdfs-site.xml
```

```
<configuration>
	<property>
		<name>dfs.namenode.http-address</name>
		<value>master:9870</value>
	</property>
	<property>
		<name>dfs.namenode.secondary.http-address</name>
		<value>slave1:9868</value>
	</property>
</configuration>
```

dfs.namenode.http-addressæ˜¯dfs namenode web uiä½¿ç”¨çš„ç›‘å¬åœ°å€å’ŒåŸºæœ¬ç«¯å£ï¼Œæˆ‘è¿™é‡Œè®¾ç½®çš„åŸºæœ¬ç«¯å£å·ä¸º9870ã€‚

dfs.namenode.secondary,http-addressåŒç†æ˜¯SecondaryNameNodeä½¿ç”¨çš„ç›‘å¬åœ°å€å’ŒåŸºæœ¬ç«¯å£ï¼Œæˆ‘è¿™é‡Œè®¾ç½®çš„åŸºæœ¬ç«¯å£å·ä¸º9868ã€‚

------

#### #yarn-site.xml

```
vi /opt/module/hadoop/etc/hadoop/yarn-site.xml
```

```
<configuration>
	<property>
		<name>yarn.resourcemanager.hostname</name>
		<value>slave2</value>
	</property>
	<property>
		<name>yarn.nodemanager.aux-services</name>
		<value>mapreduce_shuffle</value>
	</property>
</configuration>
```

yarn.reosurcemanager.hostnameï¼Œå¦‚åå­—æ‰€ç¤ºï¼Œå°±æ˜¯ç”¨äºæŒ‡å®šresourcemanagerçš„ä¸»æœºåã€‚

yarn.nodemanager.aux-serviceså±æ€§ç”¨äºæŒ‡å®šåœ¨è¿›è¡Œmapreduceä½œä¸šæ—¶ï¼Œyarnä½¿ç”¨mapreduce_shuffleæ··æ´—æŠ€æœ¯ã€‚shuffleæ˜¯MapReduceä¸­é‡è¦çš„ç»„æˆéƒ¨åˆ†ï¼Œå·¥ä½œåŸç†è¿™é‡Œå°±ä¸è¿›è¡Œèµ˜è¿°äº†ã€‚

å…·ä½“çš„å·¥ä½œåŸç†æˆ‘æ”¾åˆ°å…¶ä»–æ–‡ç« é‡Œè¿›è¡Œè®²è¿°ï¼Œè¿™é‡Œæˆ‘ä»¬å…ˆè®²é…ç½®ã€‚

------

#### #mapred-site.xml

```
vi /opt/module/hadoop/etc/hadoop/mapred-site.xml
```

```
<configuration>
	<property>
		<name>mapreduce.framework.name</name>
		<value>yarn</value>
	</property>
</configuration>
```

mapreduce.framework.nameå±æ€§ç”¨äºæ‰§è¡ŒMapReduceä½œä¸šçš„è¿è¡Œæ—¶æ¡†æ¶ã€‚å±æ€§å€¼å¯ä»¥æ˜¯localï¼Œclassicï¼Œyarnã€‚ä»€ä¹ˆæƒ…å†µä¸‹ä½¿ç”¨ä»€ä¹ˆå±æ€§å€¼ï¼Œæˆ‘ä»¬åœ¨å…¶ä»–æ–‡ç« é‡Œå†è¿›è¡Œè®²è¿°ã€‚

åœ¨hadoop2.7ç‰ˆæœ¬ä¸­ï¼Œmapred-site.xmlæ˜¯ä¸€ä¸ªæ¨¡æ¿æ–‡ä»¶ï¼Œæ–‡ä»¶åä¸ºï¼šmapred-site.xml.templateï¼Œå¦‚æœè¦è¿›è¡Œé…ç½®ï¼Œå°±å¤åˆ¶æ¨¡æ¿æ–‡ä»¶ï¼Œåœ¨å¤åˆ¶çš„æ–°æ–‡ä»¶ä¸­è¿›è¡Œé…ç½®ã€‚

**å¤åˆ¶æ¨¡æ¿æ–‡ä»¶**

```
cp mapred-site.xml.template mapred-site.xml
```

æ„ä¸ºï¼Œå¤åˆ¶æ¨¡æ¿æ–‡ä»¶ï¼Œå‘½åä¸ºï¼šmapred-site.xml

#### #hadoop-env.sh

```
vi /opt/module/hadoop/etc/hadoop/hadoop-env.sh
```

```
export JAVA_HOME=/opt/module/jdk
```

å…³äºhadoo-env.shï¼Œåœ¨hadoop2.7çš„ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä¼šé…ç½®ä¸‰ä¸ª.shæ–‡ä»¶ï¼ˆhadoop-env.shã€yarn-env.shã€mapred-env..shï¼‰çš„jdkè·¯å¾„ï¼Œä½†æ˜¯å…·ä½“ç”¨å¤„ä¸å¤ªäº†è§£ï¼ˆç­‰æˆ‘æŸ¥ä¸€æŸ¥å†è¯´ï¼‰ï¼Œåœ¨hadoop3.1.3çš„ç‰ˆæœ¬ä¸­ï¼Œæˆ‘å°è¯•è¿‡ä¸é…ç½®jdkè·¯å¾„ï¼Œå‘ç°ä¼šæŠ¥é”™ï¼Œäºæ˜¯é…ç½®äº†hadoop-env.shè¿™ä¸ªæ–‡ä»¶çš„jdkè·¯å¾„ï¼Œè§£å†³é—®é¢˜ã€‚æ‰€ä»¥ç»“è®ºæ˜¯éœ€è¦é…ç½®è¿™ä¸ªæ–‡ä»¶ï¼Œè¿™ä¸ªæ–‡ä»¶çš„å…·ä½“å«ä¹‰ç­‰æˆ‘æŸ¥äº†å†è¿›è¡Œè¡¥å……ã€‚

#### #wokers

```
vi /opt/module/hadoop/etc/hadoop/workers
```

```
master
slave1
slave2
```

wokersæ–‡ä»¶çš„åŸèº«æ˜¯æ—§ç‰ˆæœ¬ä¸­çš„slavesï¼Œåœ¨hadoop2.7ç‰ˆæœ¬ä¸­ï¼Œæ­¤æ–‡ä»¶åä¸ºslavesï¼Œåœ¨hadoop3.1.3ç‰ˆæœ¬ä¸­ï¼Œæ­¤æ–‡ä»¶åä¸ºwokersï¼Œè™½ç„¶æ›´æ”¹çš„åå­—ï¼Œä½†æ˜¯æ–‡ä»¶çš„é…ç½®å†…å®¹ä¸å‘ç”Ÿæ”¹å˜ã€‚

### @å¯åŠ¨é›†ç¾¤

#### #æ‹·è´æ–‡ä»¶

```
scp -r /opt/module/hadoop/ slave1:/opt/module
scp -r /opt/module/hadoop/ slave2:/opt/module
```

```
scp -r /etc/profile slave1:/etc/
scp -r /etc/profile slave2:/etc/
```

å°†hadoopã€ç¯å¢ƒå˜é‡æ‹·è´åˆ°ä¸¤å¤–ä¸¤ä¸ªèŠ‚ç‚¹ï¼ŒåŒæ—¶æ³¨æ„å¦å¤–ä¸¤ä¸ªèŠ‚ç‚¹è¦è¿›è¡Œç¯å¢ƒå˜é‡çš„ç”Ÿæ•ˆã€‚

#### #åˆå§‹åŒ–NameNode

```
cd /opt/module/hadoop/bin
```

```
hdfs namenode -format
hadoop namenode -format
```

ä¸¤ç§å‘½ä»¤éƒ½å¯ä»¥è¿›è¡ŒNameNodeçš„åˆå§‹åŒ–ï¼Œä¸è¿‡éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ ¹æ®ç‰ˆæœ¬ä¸åŒï¼Œæœ‰ä¸€äº›å‘½ä»¤å¯èƒ½ä¼šå‡ºç°è¢«æ·˜æ±°çš„æƒ…å†µï¼Œä¸è¿‡ç›®å‰æ¥è¯´ï¼Œå¹¶æœªç¦ç”¨ï¼Œä»ç„¶å¯ä»¥ç»§ç»­ä½¿ç”¨ï¼Œæ‰€ä»¥æ— ä¼¤å¤§é›…ğŸ¤£

> é…ä¸€å¼ æ ¼å¼åŒ–æˆåŠŸçš„å›¾ç‰‡
>
> <img src="S:\BigData\EagleHawk.github.io\image\image-20230630141110386.png" alt="image-20230630141110386" style="zoom:50%;" />
>
> å‡ºç°æ ‡çº¢å­—ä½“å³ä»£è¡¨åˆå§‹åŒ–æˆåŠŸ

éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œå¦‚æœæ›´æ”¹äº†æ ¹ç›®å½•ä¸‹çš„ä¸€äº›æ–‡ä»¶ï¼Œåˆ™éœ€è¦é‡æ–°åˆå§‹åŒ–NameNodeï¼Œä»¥ä¿è¯NamNodeçš„é…ç½®ï¼Œå¦‚pidæ–‡ä»¶ä¸ä¼šå‡ºç°é—®é¢˜ï¼Œå¦åˆ™ï¼Œé›†ç¾¤å¯èƒ½æ— æ³•å¯åŠ¨ï¼Œæ— æ³•ä½¿ç”¨ã€‚

#### #å¯åŠ¨é›†ç¾¤

```
cd /opt/module/hadoop/sbin
```

```
start-dfs.sh
start-yarn.sh
start-all.sh
```

éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œå¯åŠ¨ä¸åŒé›†ç¾¤éœ€è¦åœ¨é…ç½®çš„é‚£å°æœºå™¨ä¸Šè¿›è¡Œå¯åŠ¨ï¼Œæ¯”å¦‚æˆ‘çš„ResourceManageré…ç½®åœ¨äº†slave2èŠ‚ç‚¹ä¸Šï¼Œé‚£ä¹ˆæˆ‘å°±è¦è¿›å…¥åˆ°slave2ä¸­hadoopçš„sbinæ–‡ä»¶å¤¹ä¸‹å¯åŠ¨start-yarn.shï¼Œæ‰èƒ½æ­£å¸¸å¯åŠ¨ResourceManagerä»¥åŠNodeManagerã€‚

å…¶ä¸­ä¸‰æ¡å‘½ä»¤ï¼Œè¦ä¹ˆé€‰æ‹©start-dfs.shåŠ start-yarn.shï¼Œè¦ä¹ˆé€‰æ‹©start-all.shï¼ŒäºŒé€‰å…¶ä¸€å³å¯ã€‚

> å¯åŠ¨å®Œæˆåï¼Œå¯ä»¥é€šè¿‡jpsæ¥æŸ¥è¯¢å„èŠ‚ç‚¹è¿›ç¨‹ï¼Œæ­£ç¡®çš„è¿›ç¨‹çŠ¶æ€å¦‚ä¸‹è¡¨
>
> | èŠ‚ç‚¹å |                   æ‹¥æœ‰è¿›ç¨‹                    |
> | :----: | :-------------------------------------------: |
> | master |     NameNodeã€DataNodeã€Jpsã€NodeManager      |
> | slave1 | SecondaryNameNodeã€DataNodeã€Jpsã€NodeManager |
> | slave2 |  ResourceManagerã€DataNodeã€Jpsã€NodeManager  |
>
> è¿›ç¨‹æ‰€åœ¨èŠ‚ç‚¹æ ¹æ®ä¸åŒçš„é›†ç¾¤è§„åˆ’ä¼šåœ¨ä¸åŒçš„åœ°æ–¹ï¼Œä½†æ˜¯ä»¥ä¸Šè¿›ç¨‹å¿…é¡»æ‹¥æœ‰ï¼Œå¦‚æœæ²¡æœ‰å¯¹åº”è¿›ç¨‹ï¼Œåˆ™è¯æ˜å¯åŠ¨å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ã€æŠ¥é”™ä¿¡æ¯è¿›è¡Œé…ç½®æ–‡ä»¶çš„ä¿®æ”¹ã€‚



## Hadoopä¼ªåˆ†å¸ƒå¼å®‰è£…

**æœ‰å…³Hadoopçš„ä¼ªåˆ†å¸ƒå¼é…ç½®ï¼Œå°±ç®€å•çš„æä¸€å˜´ï¼ŒHadoopçš„ä¼ªåˆ†å¸ƒå¼æ˜¯åœ¨åªæœ‰ä¸€å°æœºå™¨æ—¶ï¼Œæ¨¡æ‹Ÿçš„é›†ç¾¤é…ç½®ï¼Œæ‰€ä»¥å’Œå®Œå…¨åˆ†å¸ƒå¼é…ç½®ä¸åŒçš„åœ°æ–¹åœ¨äºï¼Œå› ä¸ºåªæœ‰ä¸€å°æœºå™¨ï¼Œæ‰€ä»¥æ‰€æœ‰å†…å®¹å¿…é¡»æ­å»ºåœ¨åŒä¸€å°æœºå™¨ï¼Œå³æ‰€æœ‰èŠ‚ç‚¹ç«¯å£ï¼Œéƒ½é…ç½®åœ¨masterèŠ‚ç‚¹ä¸Šï¼ˆçœ‹ä½ ä»…å­˜çš„èŠ‚ç‚¹åå«ä»€ä¹ˆhhhğŸ¤£ï¼‰å³å¯ã€‚**





## Hadoop on Yarn(HA)

**å†™åœ¨æœ€å‰ï¼šå…³äºHadoopHAçš„å†…å®¹ï¼Œä¸ªäººç†è§£ä¸æ·±ï¼Œç›®å‰åªèƒ½è¾¾åˆ°èƒ½åšé¢˜ç›®çš„æ°´å¹³ï¼Œå¯¹é…ç½®é¡¹å†…çš„å‚æ•°ä¹Ÿå¹¶ä¸æ˜¯ååˆ†ç†è§£ã€‚æ­¤å¤–ï¼Œè¿˜éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå…³äºHadoopHAçš„åç»­å®‰è£…ï¼Œç›®å‰å®æµ‹åªæˆåŠŸäº†kafkaã€hiveï¼Œå¤±è´¥æŠ¥é”™flumeçš„æ•°æ®é‡‡é›†éƒ¨åˆ†ï¼ˆä»æœªè§£å†³ï¼‰ğŸ˜­ğŸ˜­ã€‚**

Hadoop çš„HAæœ‰ä¸¤ä¸ªå‰ç½®ç¯å¢ƒéœ€è¦å®‰è£…ï¼Œç¬¬ä¸€ä¸ªæ˜¯JAVAç¯å¢ƒï¼Œç¬¬äºŒä¸ªæ˜¯Zookeeperç¯å¢ƒã€‚

JAVAç¯å¢ƒåœ¨ç¬¬ä¸€ç« ä¸­æˆ‘ä»¬å·²ç»è¿›è¡Œè¿‡äº†é…ç½®ï¼Œè¿™é‡Œå°±ä¸å†èµ˜è¿°ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬æ¥é…ç½®iæ‰€éœ€è¦çš„ç¬¬äºŒç§ç¯å¢ƒâ€”â€”

### Zookeeperé…ç½®

**è§£å‹æ–‡ä»¶**

```
tar -zxvf /opt/software/zookeeper.tar.gz -C /opt/module/
```

è§£å‹å‹ç¼©åŒ…åˆ°æŒ‡å®šæ–‡ä»¶å¤¹ä¸­ã€‚

------

#### @é…ç½®ç¯å¢ƒå˜é‡

```
vi /etc/profile
```

```
export ZOOKEEPER_HOME=/opt/module/apache-zookeeper
export PATH=$PATH:$ZOOKEEPER_HOME/bin
```

```
source /etc/profile
```

#### @é…ç½®æ–‡ä»¶

##### #myid

```
cd /opt/module/apache-zookeeper
```

```
mkdir data
```

```
echo 1 > data/myid
```

å¯¹äºzookeeperï¼Œé¦–å…ˆæ˜¯æ¯ä¸ªèŠ‚ç‚¹çš„myidå€¼ï¼Œè¿™ä¸ªmyidå€¼å†è¿›è¡Œæ–‡ä»¶æ‹·è´åï¼Œéœ€è¦åœ¨ä¸åŒçš„èŠ‚ç‚¹å†™ä¸Šä¸åŒçš„å€¼ï¼Œå€¼å¿…é¡»å’Œzoo.cfgæ–‡ä»¶ä¸­server.xä¸­xçš„å€¼ç›¸åŒï¼Œæ³¨æ„å¯¹åº”èŠ‚ç‚¹ã€‚æˆ‘çš„é…ç½®ä¸‹ï¼ŒmasterèŠ‚ç‚¹ä¸º1ï¼Œslave1èŠ‚ç‚¹ä¸º2ï¼Œslave2èŠ‚ç‚¹ä¸º3ã€‚

##### #zoo.cfg

```
cd /opt/module/apche-zookeeper/conf
```

```
cp zoo_sample.cfg zoo.cfg
```

```
vi /opt/module/apache-zookeeper/zoo.cfg
```

```
dataDir=/opt/module/apache-zookeeper/data
server.1=master:2888:3888
server.2=slave1:2888:3888
server.3=slave2:2888:3888
```

é¦–å…ˆè¿›å…¥åˆ°zookeeperçš„confæ–‡ä»¶å¤¹ä¸‹ï¼Œconfæ–‡ä»¶å¤¹ä¸‹æœ‰ä¸€ä¸ªzoo_sample.cfgæ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦å¯¹è¿™ä¸ªè¿›è¡Œä¿®æ”¹ï¼Œä½†æ˜¯åŒæ ·çš„ï¼Œè¿™ä¸ªæ–‡ä»¶ä¹Ÿåªæ˜¯ä¸€ä¸ªæ¨¡æ¿æ–‡ä»¶ï¼Œæ‰€ä»¥åœ¨è¿›è¡Œä¿®æ”¹é…ç½®ä¹‹å‰ï¼Œæˆ‘ä»¬è¦å…ˆæ‹·è´æ­¤æ–‡ä»¶ã€‚

zoo.cfgæ–‡ä»¶çš„ä¿®æ”¹å†…å®¹ä¸ºä¸¤å¤„â€”â€”

**dataDir**ï¼Œè¿™æ¡å±æ€§å­˜å‚¨çš„æ˜¯zookeeperå„ä¸ªèŠ‚ç‚¹çš„myidå€¼ï¼Œè·¯å¾„è®¾ç½®ä¸ºzookeeperæ ¹ç›®å½•ä¸‹çš„dataæ–‡ä»¶å¤¹ï¼Œä¹Ÿå°±æ˜¯æŒ‡å‘myidå€¼å³å¯

**server.x=ip:2888:3888**ï¼Œè¿™æ¡å±æ€§å°±æ˜¯å¯¹åº”myidçš„å€¼ï¼Œxå³ä½ åœ¨å½“å‰èŠ‚ç‚¹ä¸‹myidæ‰€éœ€è¦é…ç½®çš„å€¼ï¼Œipå¯ä»¥æ˜¯IPåä¹Ÿå¯ä»¥æ˜¯IPåœ°å€ï¼Œå…·ä½“æ˜¯ä»€ä¹ˆï¼Œå¯ä»¥å…·ä½“æƒ…å†µéƒ½è¯•è¯•çœ‹ï¼ˆğŸ¤£ï¼‰ï¼Œå› ä¸ºæ›¾ç»å‡ºç°è¿‡IPåæŠ¥é”™ï¼Œä½†æ˜¯IPåœ°å€å¯ä»¥ä½¿ç”¨çš„æƒ…å†µï¼Œä½†æ˜¯hhhç½‘ä¸Šçš„éƒ½è¯´ä¸å¯ä»¥ç”¨IPåœ°å€ï¼Œå””ï¼Œå…·ä½“åŸå› è¿˜æ²¡æœ‰ç ”ç©¶è¿‡ï¼Œç­‰åé¢å†ç ”ç©¶äº†çœ‹ã€‚

#### @å¯åŠ¨é›†ç¾¤

```
cd /opt/module/apache-zookeeper/bin
```

```
zkServer.sh start
```

```
zkServer.sh status
```

zookeeperçš„å¯åŠ¨æ˜¯æœ‰è¯´æ³•çš„ï¼Œè¿™ä¸ªåŸç†å…³ç³»åˆ°zookeeperçš„é€‰ä¸¾æœºåˆ¶ã€‚è¿™é‡Œå°±å…ˆä¸è¿›è¡Œèµ˜è¿°äº†ã€‚ä¸»è¦åˆ†ä¸ºä¸¤ä¸ªè¿›ç¨‹â€”â€”Modeï¼šfollowerå’ŒModeï¼šLeaderã€‚

åŒæ—¶è¿˜æœ‰ä¸€ä¸ªéœ€è¦æ³¨æ„çš„åœ°æ–¹ï¼Œzookeeperçš„å¯åŠ¨éœ€è¦ä¸‰å°åŒæ—¶å¯åŠ¨ï¼Œè¿™ä¸ªå¯ä»¥å€ŸåŠ©Xshellå·¥å…·æˆ–è€…æ˜¯è‡ªä¸»ç¼–å†™è„šæœ¬ï¼Œæˆ–è€…å¦ä¸€ç§æ–¹å¼æ˜¯å•èŠ‚ç‚¹çš„ä¸€å°ä¸€å°å¯åŠ¨ã€‚å…¶ä¸­å¯åŠ¨å®è·µåº”è¯¥æ˜¯æ§åˆ¶åœ¨30000msä»¥å†…ï¼ˆè¿˜æ²¡æ‰¾åˆ°å…·ä½“çš„æ—¶é—´è¦æ±‚ï¼Œä½†æ˜¯å¾ˆä¹…ä»¥å‰å¥½åƒåœ¨å“ªé‡Œçœ‹åˆ°è¿‡ï¼‰ã€‚æ ¹æ®zookeeperçš„é€‰ä¸¾æœºåˆ¶ï¼Œç¬¬ä¸€å°å’Œç¬¬ä¸‰å°å¯åŠ¨çš„èŠ‚ç‚¹ä¸ºfollowèŠ‚ç‚¹ï¼Œç¬¬äºŒå°å¯åŠ¨çš„èŠ‚ç‚¹ä¸ºleaderèŠ‚ç‚¹ã€‚

å¯ä»¥é€šè¿‡

**æŸ¥è¯¢çŠ¶æ€**

```
zkServer.sh status
```

> å¯åŠ¨å®ŒæˆåjpsæŸ¥è¯¢
>
> | èŠ‚ç‚¹åç§° |    è¿›ç¨‹çŠ¶æ€    |
> | :------: | :------------: |
> |  master  | Modeï¼šfollower |
> |  slave1  |  Modeï¼šleader  |
> |  slave2  | Modeï¼šfollower |
>
> zookeeperæ­£å¸¸å¯åŠ¨åè¿›ç¨‹çŠ¶æ€å¦‚ä¸Šè¡¨æ‰€ç¤ºï¼Œæ³¨æ„ï¼šjpsæŸ¥è¯¢ä¸ä¼šæœ‰ç»“æœ

### HadoopHAéƒ¨ç½²

**è§£å‹æ–‡ä»¶**

```
tar -zxvf /opt/software/hadoop-3.1.3 -C /opt/module/
```

***

#### @é…ç½®ç¯å¢ƒå˜é‡

è¿™é‡Œé…ç½®çš„ç‰ˆæœ¬æ˜¯hadoop-3.1.3ï¼Œæ‰€ä»¥ç¯å¢ƒå˜é‡ä¸­å¤šå‡ºæ¥çš„ä¸ƒé¡¹ï¼Œåœ¨hadoop-2.7ç‰ˆæœ¬ä¸­ä¸éœ€è¦é…ç½®

```
vi /etc/profile
```

```
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/bin
//ä»¥ä¸‹æ˜¯hadoop-3.1.3ç‰ˆæœ¬ä¸­éœ€è¦é¢å¤–é…ç½®çš„ï¼Œè¿™äº›é…ç½®å†…å®¹è¿˜å¯ä»¥é…ç½®åœ¨å¯åŠ¨æ–‡ä»¶æˆ–è€…æ˜¯hadoop-env.shä¸­
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
//ä»¥ä¸‹æ˜¯hadoop-3.1.3ç‰ˆæœ¬ä¸­HAæ‰€éœ€è¦çš„æ–°é…ç½®å†…å®¹
export HDFS_ZKFC_USER=root
export HDFS_JOURNALNODE_USER=root
```

ä»¥ä¸Šå†…å®¹åœ¨å¯åŠ¨æ—¶ï¼Œå¦‚æœæ²¡æœ‰é…ç½®æˆ–æ˜¯é…ç½®æœ‰è¯¯ï¼Œä¼š`error`æŠ¥é”™æç¤ºï¼Œä½†æ˜¯ï¼Œè¿™é‡Œæ¨èé…ç½®å¥½ï¼Œä¸è¦åˆ°æ—¶å€™å†è¿›è¡Œä¿®æ”¹ï¼Œå¯èƒ½ä¼šå‡ºç°è§£å†³ä¸äº†æŠ¥é”™é—®é¢˜ã€‚

#### @é…ç½®æ–‡ä»¶

##### #hadoop-env.sh

```
vi /opt/module/hadoop-3.1.3/etc/hadoop/hadoop-env.sh
```

```
export JAVA_HOME=/opt/module/jdk
```

##### #core-site.xml

```
vi /opt/module/hadoop-3.1.3/etc/hadoop/
```

```
<configuration>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://mycluster</value>
	</property>
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/opt/module/hadoop-3.1.3/dfs/tmp</value>
	</property>
	<property>
		<name>ha.zookeeper.quorum</name>
		<value>master:2181,slave1:2181,slave2:2181</value>
    </property>

	<!-- ğŸ‘‡âŒ -->

	<property>
		<name>io.file.buffer.size</name>
		<value>131072</value>
    </property>
	<property>
		<name>hadoop.proxyuser.root.hosts</name>
		<value>*</value>
	</property>
	<property>
		<name>hadoop.proxyuser.root.groups</name>
		<value>*</value>
	</property>
	<property>
		<name>hadoop.proxyuser.root.users</name>
		<value>*</value>
	</property>
</configuration>
```

`fs.defaultFS`ï¼šHadoop FS å®¢æˆ·ç«¯ä½¿ç”¨çš„é»˜è®¤è·¯å¾„

`hadoop.tmp.dir`ï¼šHadoopä¸´æ—¶æ–‡ä»¶è·¯å¾„

`ha.zookeeper.quorum`ï¼šé…ç½®zké›†ç¾¤

`dfs.journalnode.edits.dir`ï¼šJournalNodeå®ˆæŠ¤è¿›ç¨‹å°†å­˜å‚¨å…¶æœ¬åœ°çŠ¶æ€çš„è·¯å¾„

`io.file.buffer.size`ï¼šSequenceFilesä¸­ä½¿ç”¨çš„è¯»/å†™ç¼“å†²åŒºçš„å¤§å°

`hadoop.proxyuser.root.hosts`ï¼šé…ç½®å…è®¸è®¿é—®çš„ä¸»æœº

`hadoop.proxyuser.root.groups`ï¼šé…ç½®å…è®¸è®¿é—®çš„ç”¨æˆ·ç»„

`hadoop.proxyuser.root.users`ï¼šé…ç½®è¿è¡Œè®¿é—®çš„ç”¨æˆ·

***

##### #hdfs-site.xml

```
vi /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml
```

```
<configuration>
	<property>
		<name>dfs.nameservices</name>
		<value>mycluster</value>
    </property>
    <property>
		<name>dfs.ha.namenodes.mycluster</name>
		<value>nn1,nn2</value>
    </property>
    <property>
		<name>dfs.namenode.rpc-address.mycluster.nn1</name>
		<value>master:8020</value>
    </property>
    <property>
		<name>dfs.namenode.rpc-address.mycluster.nn2</name>
		<value>slave1:8020</value>
    </property>
    <property>
		<name>dfs.namenode.http-address.mycluster.nn1</name>
		<value>master:9870</value>
    </property>
    <property>
		<name>dfs.namenode.http-address.mycluster.nn2</name>
		<value>slave1:9870</value>
    </property>
    <property>
		<name>dfs.namenode.shared.edits.dir</name>
		<value>qjournal://master:8485;slave1:8485;slave2:8485/mycluster</value>
    </property>
	<property>
		<name>dfs.journalnode.edits.dir</name>
		<value>/opt/module/hadoop-3.1.3/data/jn</value>
	</property>
	<property>
		<name>dfs.ha.automatic-failover.enabled</name>
		<value>true</value>
    </property>
    <property>
		<name>dfs.client.failover.proxy.provider.mycluter</name>
		<value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
    </property>
    <property>
		<name>dfs.ha.fencing.methods</name>
		<value>sshfence
			shell(/bin/bash)
		</value>
    </property>
    <property>
		<name>dfs.ha.fencing.ssh.private-key-files</name>
		<value>/root/.ssh/id_rsa</value>
    </property>

	<!-- ğŸ‘‡âŒ -->

    <property>
		<name>dfs.ha.fencing.ssh.connection-timeout</name>
		<value>30000</value>
    </property>
    <property>
		<name>dfs.datanode.data.dir</name>
		<value>/opt/module/hadoop-3.1.3/dfs/data</value>
    </property>
    <property>
		<name>dfs.blocksize</name>
		<value>268435456</value>
    </property>
    <property>
		<name>dfs.namenode.handler.count</name>
		<value>100</value>
    </property>
</configuration>
```

`dfs.nameservices`ï¼šnameservicesé€»è¾‘åç§°

`dfs.ha.namenodes.mycluster`ï¼šnameservicesæœåŠ¡ä¸­æ¯ä¸ªNameNodeçš„å”¯ä¸€æ ‡è¯†ç¬¦

`dfs.ha.namenode.rpc-address.mycluster.nnX`ï¼šnnX NameNodeç›‘å¬çš„å®Œå…¨é™å®šçš„RPCåœ°å€

`dfs.ha.namenode.http-address.mycluster.nnX`ï¼šnnX NameNodeç›‘å¬çš„å®Œå…¨é™å®šçš„HTTPåœ°å€

`dfs.namenode.shared.edits.dir`ï¼šæ ‡è¯†NameNode å°†åœ¨å…¶ä¸­å†™å…¥/è¯»å–ç¼–è¾‘çš„ JN ç»„çš„URL

`dfs.client.failover.proxy.provider.mycluster`ï¼šHDFSå®¢æˆ·ç«¯ç”¨æ¥è”ç³»Active NameNodeçš„Javaç±»

`dfs.ha.fencing.methods`ï¼šä¸€ä¸ªè„šæœ¬æˆ–JAVAç±»çš„åˆ—è¡¨ï¼Œå°†åœ¨æ•…éšœè½¬ç§»æœŸé—´ç”¨äºéš”ç¦»Active NameNode

`dfs.ha.fencing.ssh.private-key-files`ï¼šSSH åˆ° Active NameNode å¹¶ç»ˆæ­¢è¿›ç¨‹

`dfs.ha.fencing.ssh.connect-timeout`ï¼šSSHè¿æ¥è¶…æ—¶æ—¶é•¿(æ¯«ç§’)

`dfs.ha.automatic-failover.enabled`ï¼šå¼€å¯è‡ªåŠ¨æ•…éšœè½¬ç§»

`dfs.replication`ï¼šHadoopçš„å‰¯æœ¬æ•°é‡ï¼Œé»˜è®¤ä¸º3

`dfs.namenode.name.dir`ï¼šåœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿæ‰€åœ¨çš„NameNodeçš„å­˜å‚¨ç©ºé—´å’ŒæŒç»­åŒ–å¤„ç†æ—¥å¿—

`dfs.datanode.data.dir`ï¼šåœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿæ‰€åœ¨çš„DataNodeçš„å­˜å‚¨ç©ºé—´å’ŒæŒç»­åŒ–å¤„ç†æ—¥å¿—

`dfs.blocksize`ï¼šç”¨äºå¤§å‹æ–‡ä»¶ç³»ç»Ÿçš„ HDFS å—å¤§å°ä¸º 256MB

`dfs.namenode.handler.count`ï¼šNameNode æœåŠ¡å™¨çº¿ç¨‹æ¥å¤„ç†æ¥è‡ªå¤§é‡DataNode çš„ RPC

***

##### #yarn-site.xml

```
vi /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml
```

```
<configuration>
	<property>
  		<name>yarn.nodemanager.aux-services</name>
  		<value>mapreduce_shuffle</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.ha.enabled</name>
  		<value>true</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.cluster-id</name>
  		<value>RMcluster</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.ha.rm-ids</name>
  		<value>rm1,rm2</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.hostname.rm1</name>
  		<value>master</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.hostname.rm2</name>
  		<value>slave1</value>
	</property>
	<property>
  		<name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
	</property>
	<property>
  		<name>yarn.nodemanager.vmem-check-enabled</name>
  		<value>false</value>
	</property>
	<property>
  		<name>hadoop.zk.address</name>
  		<value>master:2181,slave1:2181,slave2:2181</value>
	</property>

	<!-- ğŸ‘‡âŒ -->

	<property>
  		<name>yarn.scheduler.minimum-allocation-mb</name>
  		<value>1024</value>
	</property>
	<property>
  		<name>yarn.scheduler.maximum-allocation-mb</name>
  		<value>4096</value>
	</property>
	<property>
  		<name>yarn.nodemanager.resource.cpu-vcores</name>
  		<value>5</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.ha.enabled</name>
  		<value>true</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.webapp.address.rm1</name>
  		<value>master:8088</value>
	</property>
	<property>
  		<name>yarn.resourcemanager.webapp.address.rm2</name>
  		<value>slave1:8088</value>
	</property>
</configuration>
```

`yarn.scheduler.minimum-allocation-mb`ï¼šåœ¨èµ„æºç®¡ç†å™¨ä¸­åˆ†é…ç»™æ¯ä¸ªå®¹å™¨è¯·æ±‚çš„æœ€å°å†…å­˜é™åˆ¶ã€‚ä»¥ MB ä¸ºå•ä½

`yarn.scheduler.maximum-allocation-mb`ï¼šåœ¨èµ„æºç®¡ç†å™¨ä¸­åˆ†é…ç»™æ¯ä¸ªå®¹å™¨è¯·æ±‚çš„æœ€å¤§å†…å­˜é™åˆ¶ã€‚ä»¥ MB ä¸ºå•ä½

`yarn.nodemanager.resource.cpu-vcores`ï¼šå¯ä»¥ä¸ºå®¹å™¨åˆ†é…çš„ vcore æ•°é‡ï¼Œè¿™ä¸ç”¨äºé™åˆ¶ YARN å®¹å™¨ä½¿ç”¨çš„ç‰©ç†å†…æ ¸æ•°ã€‚é»˜è®¤ä¸º8

`yarn.resourcemanager.ha.enabled`ï¼šå¯ç”¨ RM HA

`yarn.resourcemanager.cluster-id`ï¼šæ ‡è¯†é›†ç¾¤ã€‚é€‰ä¸¾äººä½¿ç”¨å®ƒæ¥ç¡®ä¿ RM ä¸ä¼šæ¥ç®¡å¦ä¸€ä¸ªé›†ç¾¤çš„æ´»åŠ¨

`yarn.resourcemanager.ha.rm-ids`ï¼šRM çš„é€»è¾‘ ID åˆ—è¡¨

`yarn.resourcemanager.hostname.rmX`ï¼šæŒ‡å®š RMé€»è¾‘id,rmXå¯¹åº”çš„ä¸»æœºå

`yarn.resourcemanager.webapp.address.rmX`ï¼šæŒ‡å®š RMé€»è¾‘id,rmXå¯¹åº”çš„webåœ°å€

`hadoop.zk.address`ï¼šæŒ‡å®šhadoopé›†ç¾¤

***

##### #mapred-site.xml

```
vi /opt/module/hadoop-3.1.3/etc/hadoop/mapred-site.xml
```

```
<configuration>
	<property>
  		<name>mapreduce.framework.name</name>
  		<value>yarn</value>
	</property>

	<!-- ğŸ‘‡âŒ -->

	<property>
  		<name>yarn.app.mapreduce.am.env</name>
  		<value>HADOOP_MAPRED_HOME=/opt/module/hadoop-3.1.3</value>
	</property>
	<property>
  		<name>mapreduce.map.env</name>
  		<value>HADOOP_MAPRED_HOME=/opt/module/hadoop-3.1.3</value>
	</property>
	<property>
  		<name>mapreduce.reduce.env</name>
  		<value>HADOOP_MAPRED_HOME=/opt/module/hadoop-3.1.3</value>
	</property>
	<property>
  		<name>mapreduce.map.memory.mb</name>
  		<value>2048</value>
	</property>
	<property>
  		<name>mapreduce.map.java.opts</name>
  		<value>-Xmx1536M</value>
	</property>
	<property>
  		<name>mapreduce.reduce.memory.mb</name>
  		<value>4096</value>
	</property>
	<property>
  		<name>mapreduce.map.java.opts</name>
  		<value>-Xmx2560M</value>
	</property>
	<property>
  		<name>mapreduce.jobhistory.address</name>
  		<value>master:10020</value>
	</property>
	<property>
  		<name>mapreduce.jobhistory.webapp.address</name>
  		<value>master:19888</value>
	</property>
	<property>
  		<name>mapreduce.jobhistory.intermediate-done-dir</name>
  		<value>/mr-history/tmp</value>
	</property>
	<property>
  		<name>mapreduce.jobhistory.done-dir</name>
  		<value>/mr-history/done</value>
	</property>
</configuration>
```

`mapreduce.framework.name`ï¼šæ‰§è¡Œæ¡†æ¶è®¾ç½®ä¸º Hadoop YARN

`mapreduce.map.memory.mb`ï¼šmapreduce.map.memory.mb

`mapreduce.map.java.opts`ï¼šmapä»»åŠ¡å­jvmçš„å †å¤§å°

`mapreduce.reduce.memory.mb`ï¼šreduceä»»åŠ¡çš„èµ„æºé™åˆ¶

`mapreduce.map.java.opts`ï¼šreduceä»»åŠ¡å­jvmçš„å †å¤§å°

`mapreduce.jobhistory.address`ï¼šMapReduce JobHistory æœåŠ¡å™¨ä¸»æœºï¼šç«¯å£

`mapreduce.jobhistory.webapp.address`ï¼šMapReduce JobHistory Server Web UIä¸»æœºï¼šç«¯å£

`mapreduce.jobhistory.intermediate-done-dir`ï¼šMapReduce ä½œä¸šå†™å…¥å†å²æ–‡ä»¶çš„ç›®å½•

`mapreduce.jobhistory.done-dir`ï¼šå†å²æ–‡ä»¶ç”± MR JobHistory Server ç®¡ç†çš„ç›®å½•

***

##### #workers

```
vi /opt/module/hadoop-3.1.3/etc/hadoop/workers
```

```
master
slave1
slave2
```

***

#### @å¯åŠ¨é›†ç¾¤

- åˆ†å‘èŠ‚ç‚¹
- å¯åŠ¨JournalNodeï¼ˆ3å°èŠ‚ç‚¹ï¼‰

```
hdfs --daemon start journalnode
```

- æ ¼å¼åŒ–namenode

```
hdfs namenode -format
```

- æ ¼å¼åŒ–zkfc

```
hdfs zkfc -formatZK
```

- å¯åŠ¨é›†ç¾¤

```
start-all.sh
```

- å¤‡ç”¨NameNodeå¤åˆ¶å…ƒæ•°æ®ç›®å½•ï¼ˆ2å°åˆ†èŠ‚ç‚¹ï¼‰

```
hdfs namenode -bootstrapStandby
```

```
hdfs --daemon start namenode
```

- æŸ¥çœ‹æ‰€æœ‰NameNodeçŠ¶æ€

```
hdfs haadmin -getAllServiceState
```

- è¿è¡ŒPiç¨‹åºæµ‹è¯•

```
yarn jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 10 10
```

- é‡å¯master namenode

```
hdfs --daemon start namenode
```

```
hdfs haadmin -getServiceState nn1
```


