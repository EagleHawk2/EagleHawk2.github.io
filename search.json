[{"title":"计算机组成原理 前言与绪论","path":"/2024/08/30/computer-organization-part1/","content":"计算机组成原理（第4版）笔记 前言与浅谈本文浅谈一下关于《计算机组成原理（第4版）》的部分理解与课堂笔记，只对笔记作补充，对具体的内容不进行讲述。 讲授这门课程的赵老师还是挺有趣的，从其他地方了解了不少他的事情。总之是一名十分具有学识和远见的老师。本文中的引用实例大部都出自赵老师之口，后经过部分查证和补充完善。自然也会有查证不实的问题，有出入的话就只能按我目前的理解来记录，待到有所理解感悟后再来进行修正。本文中使用“赵说🤓👌”来进行补充。 虽然目前为止上了4个课时，但是对这一门课也有了不少新的理解与认识，希望能结合此笔记与脑子中的课本，将这一门课程摸透，至少在形式主义上摸透这一门课程中的重点内容。 ——为之 书2024.8.29 第一章\t概论1.1\t电子计算机与存储程序控制看了一下书籍，那这里我就不按照课本分点来进行划分了，就只记下一些比较有趣的部分吧。 1\t电子计算机的分类 电子模拟计算机 处理的信息是连续变化的物理量 运算的过程是连续的 电子数字计算机 处理的信息是在时间上离散的数字量 运算的过程是不连续的 我们一般常说的计算机都是指电子计算机。 2\t计算机的发展历史一般来说发展历史都采用电子器件来进行划分，而不是按照规定你的时间 一代电子管计算机 二代晶体管计算机 三代小、中规模集成电路计算机 四代大、超大规模集成电路计算机 其实综上所述，所有高新科技的发展都是受限于材料的困难程度，要将抽象的逻辑化转为现实，所采用的材料从无到有、从原始到发达都是需要无数的实验、心血、试错才能完成科技的推动和发展。 那么计算机从电子管到晶体管再到现在的半导体，为什么能够将计算机不断发展和改进升级，这就要提到一个概念了 “制程” 制程就是制造芯片，制造什么芯片？制造半导体芯片 半导体的芯片制造需要三个方面 设计 光刻 封测 这里我们就谈谈光刻，光刻是制程的重要部分，需要用到光刻机（EUV）。 那么光刻机是什么？ 通俗来说就是用光来雕刻的工具。 雕刻本身就复杂，更何况制程的光刻，需要在7nm的芯片上进行雕刻。 7nm是什么概念？人一根头发直径是1000nm。 那么制程越高，芯片的性能就越强 扩大单位面积的电路设计 同样面积下，雕刻所需要的面积越小，就能容纳更多的电路设计，那么芯片性能自然更强。 减少所需要的电压 雕刻的间距越小，那么之间的电势差就越小，即电压越小。那么最突出的优势就是发热减少。 以小见大，众所周知，美丽国卡中国的脖子，打压华为的一大方面就是限制光刻机 在半导体制造行业中，唯一能做光刻机的企业就是Intel 诶？但是Intel能做不做，选择了性价比跟高的外包—— 外包中最顶级的自然是我们家喻户晓的亚洲四小龙之一的——台北台积电 供给有限、资源有限、生产力有限，那么为了不被永远卡脖子，那么研制我们自己的光刻机，就是高新技术行业不断努力的方向。 3\t存储程序的概念ENIAC1946年2月世界上第一台电子计算机在美国宾夕法尼亚大学诞生 ENIAC（Electronic Numerical Integrator and Computer） 话说这个ENIAC诞生的背景是二战， 制造他的目的是为了计算导弹的落点，运用合力来计算导弹的飞行轨迹。 计算机还是适合做重复性的劳动工作。 冯氏结构冯诺伊曼，美籍匈牙利裔科学家 在ENIAC研制的同时。提出了存储程序概念（1945年6月） 概括为以下三点： 计算机（指硬件）应由运算器、存储器、控制器、输入设备和输出设备五大基本部件组成 这里的存储器其实指的是一个存储器系统 计算机内部采用二进制来表示指令和数据 为什么要使用二进制而不是用其他高进制来表示指令和数据？ 首先要吧抽象的概念集转为一个物理状态下的实际输出，那么我们需要有一个物理载体 而如果我们需要使用多进制进行表示的话，那么就需要一个告诉、可控的物理载体来表示扽同意进制数的状态 目前来说，并没有发现什么材料或者是物质可以达成这样的条件 反观二进制 光、蛋白质、半导体都可以进行表示 光的明暗变化 &#x3D;》光纤 &#x3D;》光学计算机 蛋白质不同状态下的性状表现&#x3D;》生物计算机 半导体的通电1，断电0&#x3D;》电子计算机 将编好的程序和原始数据事先存入存储器中，然后再启动计算机工作 ，这就是存储程序的基本含义。 与其说启动计算机进行工作，不如说是“使用程序控制计算机工作” 那么再概括一下，冯氏结构可以分为更简单的三点内容： 存储程序 程序控制 指的注意的是，这里的程序和第一点的程序本质上是同一个东西 这个程序在存储阶段就可以事先存储在计算机中，并且在计算机中可以自己进行可控的运行&#x3D;》这就是 五大基础部件 冯氏结构实际是以运算器为中心，但是发展到现在，已转向以存储器为中心 当今计算机中，通常将运算器和控制器合称为中央计算器（Central Processing Unit，CPU） 存储器存储器可以戏称为大内总管： 主要原因是计算机内部CPU和输入设备、输出设备、外存的数据交换都需要主存的中转。 主存（main memory）+外存就构成了我们计算机当中的存储器 当然并不只是只有存储器系统当中才能进行数据的存储， 在运算器当中也有类似功能的不见，我们称为寄存器。 也就是说，公共使用的存储系统就是存储器，而固定某个系统或者是组件使用的存储系统就是寄存器。 4\t计算器部件的连接总线结构 单总线结构 特点是成本高，速度快，3GHz的CPU，总线的速度能够达到1866MHz，也就是1.8GHz。 单总线结构，CPU和主存储器主要使用系统总线，其他部件则通过接口进行分支的使用，所以速度较快。 当然相对的，成本就比较高，线路大多使用金线才能保证使用。","categories":["计算机组成原理"]},{"title":"Flink-Java | log4j语法","path":"/2024/08/13/flink-java-log4j/","content":"一）log4j根配置语法example. 1log4j.rootLogger=[level],appenderName,appenderName,... explain. 1）[level]表示日志等级 Log4j根据日志信息的重要成都，分OFF、FATAL、ERROR、WARN、INFO、DEBUG、ALL Log4j官方建议实际应用情况下，只使用四个等级，从高到低：ERROR、WARN、INFO、DEBUG ERROR为严重错误，主要是程序的错误 WARN为一般警告，比如session丢失 INFO为一般要显示信息，比如登录登出 DEBUG为程序的调试信息 根据日志分级制度，如果定义的level为INFO（标准第三等级），那么INFO以上级别的日志才显示，而DEBUG日志不会显示 使用方式 1log4j.rootLogger=[level] 2）appenderName表示日志信息输出位置（目的地） console表示输出到控制台 File表示将日志输出到文件中 DailyRollingFIle表示每天产生一个日志文件 RollingFIle表示文件大小到达指定尺寸的时候产生一个新的文件 Writer表示将日志信息以流格式发送到任意指定的地方 使用方式 1log4j.appender.appenderName = fully.qualified.name.of.appender.class 3）配置日志信息的格式 HTML表示以HTML格式形式布局 Pattern表示可以灵活的指定布局模式 Simple表示包含日志信息的级别和信息字符串 TTCC表示日志产生的时间、线程、类别等信息 使用方式 1log4j.appender.appenderName.layout = fully.qualified.name.of.layout.class","categories":["Flink"]},{"title":"Zookeeper搭建","path":"/2024/04/25/zookeeper-build/","content":"Zookeeper集群搭建解压文件#tar -zxvf zookeeper-3.4.5.tar.gz -C &#x2F;usr&#x2F;local&#x2F;src&#x2F; 解压文件到\t&#x2F;usr&#x2F;local&#x2F;src&#x2F;\t目录下 修改名字#mv zookeeper-3.4.5.tar.gz zookeepper 修改名字，方便后期配置 配置环境变量#vi &#x2F;etc&#x2F;profile 123#zookeeper ENVexport ZOOKEEPER_HOME=/usr/local/src/zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/bin #sourc &#x2F;etc&#x2F;profile 使文件生效 集群配置@新建data文件#cd &#x2F;usr&#x2F;local&#x2F;src&#x2F;zookeeper 进入zookeeper文件夹 #mdkir data 新建data文件夹 @zoo.cfg#cd &#x2F;usr&#x2F;local&#x2F;src&#x2F;zookeeper&#x2F;conf 进入zookeeper下的conf文件夹 #cp zoo_sample.cfg zoo.cfg 复制样本文件进行修改 修改内容如下，注意专注一下五个参数，其中dataDir中的地址为前一步中在zookeeper下建立的 12345678tickTime=2000initLimit=10syncLimit=5dataDir=/usr/local/src/zookeeper/dataclientPort=2181server.1=master:2888:3888server.2=slave1:2888:3888server.3=slave2:2888:3888 name 含义 tickTime tick翻译成中文的话就是滴答滴答的意思，连起来就是滴答滴答的时间，寓意心跳间隔，单位是毫秒，系统默认是2000毫秒，也就是间隔两秒心跳一次。客户端与服务器或者服务器与服务器之间维持心跳，也就是每个tickTime时间就会发送一次心跳。通过心跳不仅能够用来监听机器的工作状态，还可以通过心跳来控制Flower跟Leader的通信时间，默认情况下FL的会话时常是心跳间隔的两倍。 initLimit 集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。 syncLimit 集群中flower服务器（F）跟leader（L）服务器之间的请求和答应最多能容忍的心跳数。 dataDir 该属性对应的目录是用来存放myid信息跟一些版本，日志，跟服务器唯一的ID信息等。 clientPort 客户端连接的接口，客户端连接zookeeper服务器的端口，zookeeper会监听这个端口，接收客户端的请求访问！这个端口默认是2181。 @写入节点信息#cd &#x2F;usr&#x2F;local&#x2F;src&#x2F;zookeeper&#x2F;data 进入新建的data文件夹中 （当然这一步最好先复制后再进行，以免在分节点中没有节点信息） 使用echo语句把节点信息1、2、3分别写入三个节点的data目录中 在master节点中 #echo 1 &gt; myid 在slave1节点中 #echo 2 &gt; myid 在slave2节点中 #echo 3 &gt; myid 拷贝环境#scp -r &#x2F;usr&#x2F;local&#x2F;src&#x2F;zookeeper slave1:&#x2F;usr&#x2F;local&#x2F;src&#x2F; #scp -r &#x2F;usr&#x2F;local&#x2F;src&#x2F;zookeeper slave2:&#x2F;usr&#x2F;local&#x2F;src&#x2F; 启动集群#cd &#x2F;usr&#x2F;local&#x2F;src&#x2F;zookeeper&#x2F;bin 进入到zookeeper的bin目录下 #.&#x2F;zkServer.sh start 启动zookeeper #.&#x2F;zkServer.sh status 查看zookeeper状态 #.&#x2F;zkCli.sh 启动Cli，连接到zookeeper服务器 #.&#x2F;zkServer.sh stop 停止zookeeper服务器 关于Zookeeper的报错信息问题一关于zookeeper的配置，由于没有过多的配置因素，在检查报错信息时，只能通过排查日志文件来进行报错的修改，目前出现的问题一是zookeeper无法启动，在输入**#.&#x2F;zkServer.sh start**后，启动失败，此问题现象： 启动zk失败，但是在启动**#.&#x2F;zkServer.sh start**时，不会报错，正常显示。 jps查询时，正常显示**#QuorumPeerMain** #.&#x2F;zkServer.sh status查询时，显示报错，报错信息如下： 报错信息 #.&#x2F;zkServer.sh stop关闭集群时，主节点无法关闭的报错 解决办法： 在zoo.cfg文件中，原本版本中，查询到的是zoo.cfg文件中无法识别ip地址，故写法为： 123server.1=master:2888:3888server.2=slave1:2888:3888server.3=slave2:2888:3888 查询日志发现，报错信息为： 报错信息 显示address already in use，那就考虑是否ip名无法识别，需要使用ip地址 故进行修改zoo.cfg中ip名为ip地址 123server.1=192.168.100.3:2888:3888server.2=192.168.100.4:2888:3888server.3=192.168.100.2:2888:3888","categories":["Zookeeper"]},{"title":"Spark搭建","path":"/2024/04/25/spark-build/","content":"SparkSpark Local1tar -zxvf /opt/software/spark.tar.gz -C /opt/module 直接解压即可使用 1bin/run-example SparkPi 10 执行SparkPi案例，使用bin内置的run-example方法 Spark Standalone12345vi spark-env.shexport SPARK_MASTER_IP=masterexport SPARK_MASTER_PORT=7077export SPARK_WORKER_CORES=1export SPARK_WORKER_MEMORY=3g 可以根据文件中的提示来自行配置所需内容 1234vi workersmasterslave1slave2 12vi sbin/spark-config.shexport JAVA_HOME=/opt/module/jdk 在sbin目录下要加入jdk路径，防止JAVA相关报错 1234bin/spark-submit \\--class org.apache.spark.examples.SparkPi \\--master spark://master:7077 \\example/jars/spark-examles_2.12-3.3.0.jar 10 执行案例 Spark on Yarn解压文件 1tar -zxvf /opt/software/spark -C /opt/module/ @配置环境1234vi /etc/profileexport SPARK_HOME=/opt/module/sparkexport PATH=$PATH:$SPARK_HOME/binsource /etc/profile @配置文件#spark-config.sh(sbin)12vi /opt/module/spark/sbin/spark-config.shexport JAVA_HOME=/opt/module/jdk 注意这里是进入sbin目录下的spark-config.sh中进行配置jdk #spark-env.sh12345678cp spark-env.sh.template spark-env.shvi /opt/module/spark/conf/spark-env.shexport JAVA_HOME=/opt/module/jdkexport SPARK_HOME=/opt/module/sparkexport SPARK_LOCAL_DIR=$&#123;SPARK_HOME&#125;/tmp#on yarn配置export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopexport YARN_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop SPARK_LOCAL_DIR配置的路径为spark的tmp目录； HADOOP_CONF_DIR配置的路径为hadoop的conf（hadoop下的etc&#x2F;hadoop） YARN_CONF_DIR配置的路径为hadoop的conf（hadoop下的etc&#x2F;hadoop） #workers123vi /opt/module/spark/workersslave1slave2 同样的，这个文件对应老版本的slaves文件，新版问为workers，配置内容相同 #yarn-site.xml在hadoop&#x2F;etc&#x2F;hadoop下 spark on yarn配置时，需要在hadoop下的yarn文件中添加两行配置，在hadoop中有讲解 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动与使用@分发节点与启动集群#分发节点12345scp -r /opt/module/spark slave1:/opt/modulescp -r /opt/module/spark slave2:/opt/modulescp -r /etc/profile slave1:/etc/scp -r /etc/profile slave2:/etc/source /etc/profile #启动集群1start-all.sh @Spark Shell#进入shell1spark shell #运行架包1spark-submit --master yarn --class org.apache.spark.examples.SparkPi $SPARK_HOME/examples/jars/spark-examples_2.12-3.1.1.jar","categories":["Spark"]},{"title":"Redis搭建","path":"/2024/04/25/redis-build/","content":"Redis解压文件 1tar -zxvf /opt/software/redis.tar.gz -C /opt/module/ 查看Linux系统有没有gcc环境 1gcc -version 安装gcc 1yum install gcc-c++ 编译redis 12cd /opt/module/redis-6.2.6make 安装redis 1make install PREFIX=/opt/module/redis-6.2.6 拷贝conf 1cp redis.conf /opt/moduele/redis-6.2.6/bin 启动redis 前台启动 1./bin/redis-server redis.conf 直接运行的时候设置参数 1./bin/redis-server --port 6388 后台启动，使用配置文件 备份 redis.conf , 拷贝一份 redis.conf 到其他目录。 如让在 &#x2F;etc 目录下，cp redis.conf &#x2F;etc&#x2F;redis.conf修改配置文件， 将 daemonize no 改成 yes， 让服务在后台启动redis 后台启动： redis-server &#x2F;etc&#x2F;redis.conf 将启动固定为启动命令 1234567891011121314# cat /usr/lib/systemd/system/redis.service[Unit]Description=RedisAfter=syslog.target network.target remote-fs.target nss-lookup.target [Service]Type=forkingPIDFile=/run/redis_6379.pidExecStart=/usr/local/redis-6.2.4/src/redis-server /usr/local/redis-6.2.4/redis.confExecStop=/bin/kill -s QUIT $MAINPIDPrivateTmp=true [Install]WantedBy=multi-user.target 查看redis进程 1ps -ef | grep redis","categories":["Redis"]},{"title":"CentOS与大数据集群搭建思路","path":"/2024/04/25/linux-build/","content":"前言本章讲解的是在搭建Hadoop生态系统前需要的一些思路和条件，内容冗余，多是些吐槽的话，大概看看内容是否都已经配置了即可。 Hadoop 生态是学习大数据的重中之重，虽然我理解不深，但是关于hadoop所衍生出的apache的顶级开源项目，都是围绕着hadoop而进行使用的，所以要想使用apache的其他顶级项目，首先要理解的就是关于Hadoop的一些知识。 由于我在学习的时候，是采用实例式学习，于是乎，先是会用，才是理解项目，所以有很多东西，是自己的理解，不能和真正的学院派相媲美，所以此文章更多的是用于自己搭建训练的时候使用，一些问题也是相关与自身问题而撰写，分门别类，各位读者各取所需即可 （EagleHawk 2023.6.28） 前置安装搭建规划写在最前——是因为写到第三点了，结果发现规划应该是坐在所有步骤之前的，好吧只能写在最前了TAT 如何规划集群搭建： 首先要了解hadoop的工作原理。 hadoop有很多子项目，在规划时，我（个人哈o(￣▽￣)ブ）只考虑两个模块，一个是HDFS分布式文件系统，另一个是MapReduce分布式计算框架。这两个子项目是hadoop当中相当吃资源的两个项目，所以一般是需要讲这两个模块部署在不同的节点上面（即，hdfs部署在一台虚拟机上，mapreduce部署在一台虚拟机上）。 以上，就是简单的搭建规划了，在搭建规划时，我们还需要规划ip名，ip地址等，这里就不赘述了，直接实操就能理解（好吧，因为我也是实操练出来的…拳打百遍，其意自现嘛hhh）以下，是个人的部分理解。 HDFS存储架构下，有三个进程：NameNode，DataNode，SecondaryNameNode MapReduce计算框架下，有两个进程：ResourceManger，NodeManager 其中SecondaryNameNode和ResourceManger都是占资源的大户，所以一般来说，会将这两个进程部署在不同的节点，作为完全分布式的部署。如果将所有进程都部署在相同节点下，则是伪分布式的安装配置。 节点名称 IP规划 分布式规划 master 192.168.xx.xx NameNode、DataNode slave1 192.168.xx.xx SecondaryNameNode slave2 192.168.xx.xx ResourceManager 分析好了节点分布，那么就可以开始部署环境了——(ง •_•)ง 环境条件 名称 媒介 版本 备注 Linux 虚拟机 CentOS7 3台（因为搭建的是集群，所以需要三台虚拟机） jdk tar.gz压缩包 jdk-8u212-linux-x64.tar.gz jdk版本需要对应，可以进apche官网查看 hadoop tar.gz压缩包 hadoop-3.1.3.tar.gz zookeeper tar.gz压缩包 apache-zookeeper-3.5.7-bin.tar.gz zookeeper在配置HadoopHA时需要使用 在hadoop3.1.3版本之前，我进行过hadoop2.7版本的搭建，相比之下，hadoop版本更改了一些配置文件，同时也增加了一些需要配置的内容，在下文中会详细进行简述，但是大差不差，所有的配置都有内部的核心文件与参数，更改这些核心参数，就是集群搭建的核心。 虚拟机在配置开始前，需要对每一台虚拟机做三个文件的了解或是配置 hostname 1234vi /etc/hostnamemasterslave1slave2 此文件存储着本机的ip名，可以进行修改，修改方式按照规划集群来进行修改，主节点master，分节点slave1，slave2。 ifcfg-ens33 1vi /etc/sysconfig/network-script/ifcfg-ens33 TYPE&#x3D;”Ethernet”（网络类型：英特网）BOOTPROTO&#x3D;”static”（网络配置参数：静态IP&#x2F;dhcp 动态IP&#x2F;none 无）NAME&#x3D;”ens33”（网络属于网卡：ens33）DEVICE&#x3D;”ens33”（网络名称：ens33）ONBOOT&#x3D;”yes”（开机自启动：yes&#x2F;no）IPADDR&#x3D;”192.168.48.200”（本机IP）PREFIX&#x3D;”24”（子网掩码的位数）GATEWAY&#x3D;”192.168.48.2”（网关）DNS1&#x3D;”192.168.48.2”（域名） 这个文件是存储本机ip地址的文件，在不同的版本中，存储位置也会有变动，目前我见过的存储位置就两个文件，这个文件是OS7当中的存储位置，修改ip即在此文件中进行修改。 查询ip地址也可以用以下指令。 查询ip 12ip addrifconfig 如果第二条指令报错为未安装工具包，安装命令 安装net-tools工具包 1yum -y install net-tools hosts 1234vi /etc/hosts192.168.xx.xxx master192.168.xx.xxx slave1192.168.xx.xxx slave2 存放各个节点映射，ip名为本机ip名，ip地址为本机ip地址。 注意 修改完上述文件后，需要重启网络服务，重启机器 重启网络服务 1service network restart 重启机器 1reboot 防火墙关于防火墙的问题，在进行集群搭建的时候，防火墙是一定需要关闭的东西。因为搭建的集群都是内网搭建的，对外还有一个服务器，那个服务器有防火墙，由它来访问内网集群，如果内网内开启防火墙，内网集群通讯会出现很多问题。关闭防火墙，一劳永逸！ 这是在测试环境下需要关闭防火墙，在生产环境目前还没有进行过生产，所以只针对与当前测试版本 另外，以下shell代码都是在CentOS7版本下的shell命令，如果版本不同，请移步baidu.com 查看防火墙状态 1systemctl status firewalld 暂时关闭防火墙 1systemctl stop firewalld 永久关闭防火墙 1systemctl disable firewalld 重启防火墙 1systemctl enabled firewalld SSHSSH是一种加密的网络传输协议，可以在不安全的网络中为网络服务提供安全的传输环境。SSH通过在网络中建立安全隧道来实现SSH客户端与服务器之间的连接。SSH最常见的用途是远程登录系统，人们通常利用SSH来传输命令行界面和远程执行命令。SSH使用频率最高的场合是类Unix系统，但是Windows操作系统也能有限度地使用SSH。2015年，微软宣布将在未来的操作系统中提供原生SSH协议支持，Windows 10 1803版本已提供OpenSSH工具。在设计上，SSH是Telnet和非安全shell的替代品。 通过SSH Client我们可以连接到运行了SSH Server的远程机器上。 而每台机器都可以生成自己的安全密钥，而我们在搭建集群时，为了方便ssh的服务切换，会进行ssh的免密登录。以此来快速进行SSH Server切换。 如果不考虑特殊情况，可以直接看免密登录的方式，其他作为了解即可。 查看SSH服务是否启动 1systemctl status sshd @Action:active(running)SHH运行中 @Connection refusedSSH未安装 启动SSH服务 1systemctl start sshd 安装OpenSSH Server 1sudo apt-get install openssh-server 接下来介绍SSH Client公共密钥登录，其中，-p port为监听的端口号，一般为22，如果为22，可省略；user为用户名，ip可以时IP、域名、别名 查询用户名 1whoami SSH公共钥登录 12ssh -p 22 user@ipssh user@ip -p port 直接跳转到重点——免密登录，这个是我们搭建集群需要做的一个关键步骤，可以极大的方便我们进行节点的切换、拷贝、部署，所以必须要掌握，也推荐使用。 生成SSH密钥 1ssh-keygen 密钥生成后，存储位置为： ~&#x2F;.ssh&#x2F;id_rsa.pub\t公钥存储地址 ~&#x2F;.ssh&#x2F;id_rsa 私钥存储地址 复制密钥到分节点中 12ssh-copy-id masterssh-copy-id user@ip -p port Mac安装ssh-copy-id 1brew install ssh-copy-id 此条用于没有安装ssh-copy-id命令时，使用的安装命令，仅限于Mac（额，和这个搭建没什么关系，就是一个补充的说明） Windows手动指令 1ssh user@ip -p port ‘mkdir -p .ssh&amp;&amp; cat &gt;&gt; .ssh/authorized_keys’ &lt; ~/.ssh/id_rsa.pub 或者手动操作，复制公钥内容，然后登入远程机器，粘贴到 ~&#x2F;.ssh&#x2F;id_rsa.pub 公钥存储地址 .ssh&#x2F;authorized_keys存储其他公钥的位置 SSH配置别名 12345vi ~/.ssh/configHost labHostName remoteUser userPort port 个人目前没有使用过，但是在ssh其他服务当中有介绍ssh别名的使用方法，需要的话也可以记一下。 完成了免密登录，接下来介绍一下SSH传输文件服务，使用此命令可以极大的提高文件在集群内传输的效率——前提是需要做好上面的步骤。 传输文件夹 1scp -r /path/file-titele/file user@ip:/path/file-titele 使用别名 1scp port /path/to/local/file lab:/path/to/local/file 将远程文件下载到本地 1scp lab:port /path/to/local/file /path/to/local/file 将本地文件传输到远程 1scp -P port /path/to/local/file user@ip：/path/to/local/file 其中，port为端口号，path为路径，file为文件名，传输时如果传输同一地址，在被传输的文件夹下，传输路径是传输者的上一级文件（因为文件还没传过去hhh,这句话就是废话） 以上使用的方法都大差不差，主要的区别就是修改了参数达成不同的效果，如果要详细了解各参数的含义，可以搜索以下，这里我就只贴张图，不做其他说明，需要的自取就好啦。 传输文件 JDK因为Hadoop的生态环境，都是以Java为底层逻辑编写的，所以在配置环境前，需要做的一个必要配置，就是配置JDK。这是配置Hadoop的前置要求，也是Hadoop集群生态圈的底层逻辑。 解压文件到指定目录 1tar -zxvf /opt/software/jdk.tar.gz -C /opt/module/ 修改环境变量 123vi /etc/profileexport JAVA_HOME=/opt/module/jdkexport PATH=$PATH:$JAVA_HOME/bin 结语","categories":["Linux"]},{"title":"Kafka搭建","path":"/2024/04/25/kafka-build/","content":"Kafka安装配置Zookeeper安装配置在配置Kafka之前，需要安装zookeeper 解压文件 1tar -zxvf /opt/software/zookeeper.tar.gz -C /opt/module/ @配置环境变量1234vi /etc/profileexport ZOOKEEPER_HOME=/opt/module/apache-zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/binsource /etc/profile @配置文件#myid123cd /opt/module/apache-zookeepermkdir dataecho 1 &gt; data/myid #zoo.cfg1234567cd /opt/module/apche-zookeeper/confcp zoo_sample.cfg zoo.cfgvi /opt/module/apache-zookeeper/zoo.cfgdataDir=/opt/module/apache-zookeeper/dataserver.1=master:2888:3888server.2=slave1:2888:3888server.3=slave2:2888:3888 @启动集群123cd /opt/module/apache-zookeeper/binzkServer.sh startzkServer.sh status 查询状态 1zkServer.sh status 启动完成后jps查询 节点名称 进程状态 master Mode：follower slave1 Mode：leader slave2 Mode：follower zookeeper正常启动后进程状态如上表所示，注意：jps查询不会有结果 Kafka安装配置当zookeeper配置完成后，即可开始配置kafka @配置环境变量123vi /etc/profileexport KAFKA_HOME=/opt/module/kafkaexport PATH=$PATH:$KAFKA_HOME/bin @配置文件在配置文件时，有两个思路，一个 是启用kafka自带的zookeeper，另一个是启动zookeeper后单独启动kafka。在这里我使用的是第二种方法，第一种方法的配置内容在文末再进行说明。 #server.properties1234broker.id=1listeners=PLAINTEXT://192.168.100.3:9092log.dirs=/opt/module/kafka/kafka-logszookeeper.connect=master:2181,slave1:2181,slave2:2181 broker.id为自定义的数字，可以自己设定，但是需要注意的是，kafka集群中每台节点的id值不可以相等。 listeners为监听地址，配置主节点的ip地址即可。 log.dirs为kafka的日志文件路径，路径自定义即可，日志文件夹会自动创建。 zookeeper.connect配置为三台节点的zookeeper端口和ip名称即可。 如果不适用自带的zookeeper，上述命令即可完成kafka搭建。 下面的内容是使用自带的zookeeper需要配置的。 #data&#x2F;zk这里的配置方法和zookeeper中的相同，也是在根目录下新建文件夹，然后在文件夹中写入myid，注意各节点的myid值需要对应server后面的值。同时，后面dataDir对应的路径名也是myid对应的路径名。 #zookeeper.properties123456789vi /opt/module/kafka/config/zookeeper.propertiestickTime=2000initLimit=10syncLimit=5clientPort=2181dataDir=/opt/moduele/kafka/data/zkserver.1=192.168.100.3:2888:3888server.2=192.168.100.4:2888:3888server.3=192.168.100.2:2888:3888 此文件的配置内容和zookeeper中zoo.cfg内容相同，唯一需要注意的是zoo.cfg的模板文件会添加好前几条内容作为默认配置，但是zookeeper.properties中需要自己手动添加。 @启动集群拷贝集群 12345scp -r /opt/module/kafka slave1:/opt/module/scp -r /opt/module/kafka slave1:/opt/module/scp -r /etc/profile slave1:/etc/scp -r /etc/profile slave2:etc/source /etc/profile 更改配置 记得在从节点上更改broker.id和listeners监听端口的ip地址（虽然我之前好行忘记改ip地址也正常启动了，但是不知道不配置会不会有什么影响，还是配上吧哈哈。） 启动Zookeeper（内置） 1./bin/zookeeper-server-start.sh config/zookeeper.properties &amp; 启动Kafka 1./bin/kafka-server.start.sh config/server.properties &amp; 停止zk&#x2F;kafka 12./kafka-server-stop.sh./zookeeper-server.stop.sh 再开其实要先开启zk再开启kafka，同理关闭的时候也应该先关闭kafka再关闭zk（应该用不着） @JPS启动完成后，需要进行查询状态进程来确认是否成功启动了zk和kafka 1jps 节点名称 进程状态 master QuorumPeerMain、kafka slave1 QuorumPeerMain、kafka slave2 QuorumPeerMain、kafka QuorumPeerMain进程为zookeeper成功启动后的进程 kafka为kafka成功启动后的进程 需要强调的是，如果创建kafka-topic中replication-facor（副本数）大于1的话，需要启动分节点的kafka进程，如果不启动分节点的kafka，则无法创建，会报错broker值小于副本数。 Kafka-shell简单的创建topic kafka-topic创建方法有两个 需要强调的是，如果创建kafka-topic中replication-facor（副本数）大于1的话，需要启动分节点的kafka进程，如果不启动分节点的kafka，则无法创建，会报错broker值小于副本数。 bootstrap1./bin/kafka-topics.sh --bootstrap-server localhost:9092(,loclalhost1:9092,localhost2:9092) --list –list\t查询集群中是否有topic –bootstrap-server localhost:9092\t连接服务器，localhost为本集群名称，9092为端口号，测试环境下写一个即可，生产环境则2-3个 1./bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic name --create --partitions 1 --replication-factor 3 –topic name 选中topic，name为topic名字，name可自定义更改 –create 创建topic –partitions 1 设置分区，1可以改为自定义分区 –replication-factor 3 设置副本数，3可以改为自定义副本数 1./bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic name --describe -topic name\t选中topic，name为topic名字，name可自定义更改 –describe\t查询name topic的详细信息 zookeeper1bin/kafka-topics.sh --zookeeper localhost:2181 --topic name --create --partitions 1 --replication-factor 3 两个方法不一样的地方是再topic后面使用的指令，我现在使用的是--zookeeper，没有使用--bootstrap-server的原因是后者创建topic成功没有提示（……但是我就是要那个提示） bootstrap和zookeeper的区别我们使用的kafka版本为2.x版本，而**#–bootstrap-server**为kafka3.x版本的指令 Kafka开发团队重写了ZooKeeper的Quorum控制器代码并嵌入到Kafka中。所以从v2.8版本开始，Kafka不再依赖ZooKeeper。 所以， 旧版： 1--zookeeper localhost:2181 新版： 1--bootstrap-server localhost:9092 其中，2181是ZooKeeper的监听端口，9092是Kafka的监听端口。 旧版用–zookeeper参数，主机名（或IP）和端口用ZooKeeper的，也就是server.properties文件中zookeeper.connect属性的配置值 新版用–bootstrap-server参数，主机名（或IP）和端口用某个节点的即可，即主机名（或主机IP）:9092。 附表（需要的自取）： kafka shell order explain 作用 –alter Alter the number of partitions,replication assignment,and&#x2F;or configuration for the topic. 改 –bootstrap-server&lt;String: server to connect to&gt; REQUIRED:The Kafka server to connect to. 连接 –topic&lt;String: topic&gt; The topic to create, alter,describe or delete.It also accepts a regular expresstion,except for –create option.Put topic name in double quotes and use the ‘ \\ ‘ prefix to escape regular expression symbols; e.g.”test \\ .topic”. topic –create Create a new topic. 增 –delete Delete a topic 删 –list List all available topics 查 –describe List details for the given topics 查 –partitions&lt;Integer: # of partitions&gt; The number of partitions for the topic being created or altered(WARNING:If partitions are increased for a topic that has a key,the partition logic or ordering).If not supplied for create,defalts to the cluster default. 分区 –repliction-factor&lt;Integer:replication factor&gt; The replication factor for each partition in the topic being created.If not supplied,defaults to the cluster default. 副本","categories":["Kafka"]},{"title":"Hive搭建","path":"/2024/04/25/hive-build/","content":"Hive配置MySQL配置mysql的配置比较复杂和麻烦，建议单独配置好了再进行下一步（虚拟机配置mysql成功后记得多拷贝几份！！！） Hive配置解压文件 1tar -zxvf /opt/software/apache-hive -C /opt/module @配置环境1234vi /etc/profileexport HIVE_HOME=/opt/module/apache-hiveexport PATH=$PATH:$HIVE_HOME/binsource /etc/profile @配置文件#hive-site.xml12345678910111213141516171819vi /opt/module/apache-hive/conf/hive-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hive-site.xml配置文件可以由hive-default.xml.tamplate模板文件复制而来，也可以直接使用 vi来进行书写创建。基础的hive配置只需要完成上述四条的配置即可。 ConnectionURL，mysql数据库的访问路径，没有路径则总动创建； ConnectionDriverName，连接mysql数据库的驱动； ConnectionUserName，连接mysql数据库的用户名； ConnectionPassword，连接mysql数据库的秘密。 #hive-env.sh12345vi /opt/module/apache-hive/conf/hive-env.shexport JAVA_HOME=$&#123;JAVA_HOME&#125;export HIVE_HOME=$&#123;HIVE_HOME&#125;export HADOOP_HOME=$&#123;HADOOP_HOME&#125;export HIVE_CONF_DIR=$&#123;HIVE_HOME&#125;/conf 这里需要配置 jdk、hadoop、hive、hive/conf四条路径信息 @架包#mysql-connect.jar.gz1cp /opt/software/mysql-connection.jar.gz /opt/module/apache-hive/lib mysql的连接架包，在老版本中，给出的mysql架包是一个压缩包，其中有两个架包需要移动； 而在新版本中，只需要移动这一个架包即可。 #guava.jar1cp /opt/module/hadoop/share/hadoop/common/lib/guava2.7.jar /opt/module/apache-hive/lib guava.jar是hadoop3.1.3新版本中的问题，是apache项目中对应的版本无法和hadoop版本对应导致的问题，所以当前版本报错，需要把hadoop下的2.x版本的架包移动到对应项目的lib文件夹下，并且删除以前的旧版本，即可完成。 删除guava1.x.jar 1rm -rf /opt/module/apache-hive/lib/guava.1.x.jar 删除老版本架包再运行。 @初始化与启动#初始化1schematool -dbType mysql -initSchema 这个版本的hive初始化时，会出现大片空白，经过咨询，是正常情况。 1schematool -dbType mysql -initSchema --verbose 补充，使用以上指令时候，即加上--verbose则正常显示，不会出现空白bug。 #启动1hive --service metaservice &amp;","categories":["Hive"]},{"title":"Hbase搭建","path":"/2024/04/25/hbase-build/","content":"Hbase解压文件 1tar -zxvf /opt/software/hbase -C /opt/module/ 配置环境12345vi /etc/profileexport HBASE_HOME=/opt/module/hbaseexport PATH=$PATH:$HBASE_HOME/binepxort HADOOP_CLASSPATH=$&#123;HADOOP_HOME&#125;/lib/*source /etc/profile 第三条好像不需要进行配置，实测下来不配第三条也能正常运行 配置文件@hbase-env.sh123vi /opt/module/hbase/hbase-env.shexport JAVA_HOME=/opt/module/jdkexport HBASE_MANAGES_ZK=false HBASE_MANAGES_ZK含义是不使用hbase自带的zookeeper @hbase-site.xml12345678910111213141516171819202122232425vi /opt/module/hbase/hbase-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://master:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master:2181,slave1:2181,slave2:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/opt/module/hbase/data&lt;/value&gt; &lt;/property&gt; //不配置此条命令会导致HMaster进程无法启动 &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hbase.rootdir中的value值需要和hadoop中core-site.xml中配置的端口号相同，后面的hbase可以自定义； hbase.zookeeper.property.dataDir记录的是存储日志文件的路径； hbase.unsafe.steam.capability.enforce，注意这条命令，经过多次测试，此版本的hbase如果不配置此条指令，会导致hbase启动后，HMaster自动ban掉，进入hbase-shell后输入指令报错！！！ hbase.zookeeper.quorum中可以只写master、slave1、slave2三个值，但是如果这么写的话，需要再加一条prot来记录zookeeper的端口号。 1234&lt;property&gt; &lt;name&gt;hbase.zookeeper.prot&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt;&lt;/property&gt; @regionservers1234vi /opt/module/hbase/conf/regionserversmasterslave1slave2 regionservers中三台节点都需要写入，regionservers用于启动HRegionServer进程，所以，不可以只写分节点。如果只写分节点，会导致无法正常启动HRegionServer进程。 分发节点与启动集群@分发节点12345scp -r /opt/module/hbase slave1:/opt/modulescp -r /opt/module/hbase slave2:/opt/modulescp -r /etc/profile slave1:/etc/profilescp -r /etc/profile slave2:/etc/profilesource /etc/profile @启动集群1start-hbase.sh 在主节点启动即可。 Hbase Shell进入hbase 1hbase shell 查看命名空间 1list_namespace 部分报错 HA组件下 HA组件下，Hbase-2.2.3版本的启动会出现HRegionServer进程无法启动，根据报错推断是有关namenode的逻辑名有关的报错 解决方法： 12cp /opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml /opt/module/hbase-2.2.3/confcp /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml /opt/module/hbase-2.2.3/conf 将Hadoop中core-site和hdfs-site文件copy到hbase的conf文件夹下，在进行启动即可正常启动。","categories":["Hbase"]},{"title":"Hadoop搭建","path":"/2024/04/25/hadoop-build/","content":"Hadoop搭建写在最前，本文中所使用的配置是可以达到使用hadoop集群搭建的最基本配置，如果有其他需求，按照自己的需求去增加或删除配置文件即可，本文只阐述最基本用例，后期优化就靠自己啦~加油噢💪 Hadoop完全分布式安装解压到指定文件夹 1tar -zxvf /opt/software/hadoop.tar.gz -C /opt/module/ 每一个文件的配置无外乎就两个配置内容，一个是环境变量的配置，一个是文件内的配置。而要使用的第一步，自然就是安装。我们采用的安装方法是压缩包安装，所以我们安装的第一步，是解压文件 @配置环境变量123vi /etc/profileexport HADOOP_HOME=/opt/module/hadoopexport PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin 以上是hadoop2.7版本中环境变量需要配置的内容。 在这里再提一个Linux shell语法，可以快速查询路径： 查询当前所在路径名 1pwd 但是在hadoop的3.1.3的版本中，需要加入以下内容，否则会报错。幸运的是，报错内容也会提示你如何去修改环境变量。（这是第一种配置方式，文章结尾的补充说明中，我会讲述第二种配置方式🤣当然我还是觉得这种配置起来更方便啦） 12345export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=rootexport YARN_RESOURCEMANAGER_USER=rootexport YARN_NODEMANAGER_USER=root hadoop3.1.3版本需要加入以上内容。 全局环境变量生效 1source /etc/profile 每次修改完环境变量需要对环境变量进行生效命令。 关于环境变量的生效，再补充一些内容。 如果当出现全局变量配置出现问题导致基础命令失效的情况—— 有一个最高位的命令可以使用： 全局变量修改 1/bin/vi /etc/profile 全局变量生效 1./etc/profile 以上方法无法解决直接输入 1export PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin 或者 使用最高位指令进入&#x2F;etc&#x2F;profile后输入 1export PATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin @配置文件配置文件模板 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;&lt;/name&gt; &lt;value&gt;&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 没啥用，存一个，方便复制粘贴。 #core-site.xml1234567891011vi /opt/module/hadoop/etc/hadoop/core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; NameNode内部通信的端口在hadoop3.1.3版本中有三个常用端口号：8020，9000，9820，我常用的端口号是9000。需要注意的是，Clickhouse的默认端口号也是9000，所以当需要配置两个的时候，有一个需要更改端口号，否则会出现占用的情况。 hadoop.tmp.dir存储的是hadoop文件系统依赖，它有默认位置在根目录的&#x2F;tmp下，但是存储于默认位置下的tmp文件是临时文件，在机器进行开关时会重置丢失文件，所以我们需要手动为它配置一个路径。我写的data文件夹是安装包不自带的，data文件在执行NameNode的初始化时会自动生成，也可以自己为它创建。 新建data文件夹 1mkdir data #hdfs-site.xml1234567891011vi /opt/module/hadoop/etc/hadoop/hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address&lt;/name&gt; &lt;value&gt;master:9870&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;slave1:9868&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; dfs.namenode.http-address是dfs namenode web ui使用的监听地址和基本端口，我这里设置的基本端口号为9870。 dfs.namenode.secondary,http-address同理是SecondaryNameNode使用的监听地址和基本端口，我这里设置的基本端口号为9868。 #yarn-site.xml1234567891011vi /opt/module/hadoop/etc/hadoop/yarn-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;slave2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn.reosurcemanager.hostname，如名字所示，就是用于指定resourcemanager的主机名。 yarn.nodemanager.aux-services属性用于指定在进行mapreduce作业时，yarn使用mapreduce_shuffle混洗技术。shuffle是MapReduce中重要的组成部分，工作原理这里就不进行赘述了。 具体的工作原理我放到其他文章里进行讲述，这里我们先讲配置。 #mapred-site.xml1234567vi /opt/module/hadoop/etc/hadoop/mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapreduce.framework.name属性用于执行MapReduce作业的运行时框架。属性值可以是local，classic，yarn。什么情况下使用什么属性值，我们在其他文章里再进行讲述。 在hadoop2.7版本中，mapred-site.xml是一个模板文件，文件名为：mapred-site.xml.template，如果要进行配置，就复制模板文件，在复制的新文件中进行配置。 复制模板文件 1cp mapred-site.xml.template mapred-site.xml 意为，复制模板文件，命名为：mapred-site.xml #hadoop-env.sh12vi /opt/module/hadoop/etc/hadoop/hadoop-env.shexport JAVA_HOME=/opt/module/jdk 关于hadoo-env.sh，在hadoop2.7的版本中，我会配置三个.sh文件（hadoop-env.sh、yarn-env.sh、mapred-env..sh）的jdk路径，但是具体用处不太了解（等我查一查再说），在hadoop3.1.3的版本中，我尝试过不配置jdk路径，发现会报错，于是配置了hadoop-env.sh这个文件的jdk路径，解决问题。所以结论是需要配置这个文件，这个文件的具体含义等我查了再进行补充。 #wokers1234vi /opt/module/hadoop/etc/hadoop/workersmasterslave1slave2 wokers文件的原身是旧版本中的slaves，在hadoop2.7版本中，此文件名为slaves，在hadoop3.1.3版本中，此文件名为wokers，虽然更改的名字，但是文件的配置内容不发生改变。 @启动集群#拷贝文件1234scp -r /opt/module/hadoop/ slave1:/opt/modulescp -r /opt/module/hadoop/ slave2:/opt/modulescp -r /etc/profile slave1:/etc/scp -r /etc/profile slave2:/etc/ 将hadoop、环境变量拷贝到两外两个节点，同时注意另外两个节点要进行环境变量的生效。 #初始化NameNode123cd /opt/module/hadoop/binhdfs namenode -formathadoop namenode -format 两种命令都可以进行NameNode的初始化，不过需要注意的是，根据版本不同，有一些命令可能会出现被淘汰的情况，不过目前来说，并未禁用，仍然可以继续使用，所以无伤大雅🤣 配一张格式化成功的图片 出现标红字体即代表初始化成功 需要说明的是，如果更改了根目录下的一些文件，则需要重新初始化NameNode，以保证NamNode的配置，如pid文件不会出现问题，否则，集群可能无法启动，无法使用。 #启动集群1234cd /opt/module/hadoop/sbinstart-dfs.shstart-yarn.shstart-all.sh 需要说明的是，启动不同集群需要在配置的那台机器上进行启动，比如我的ResourceManager配置在了slave2节点上，那么我就要进入到slave2中hadoop的sbin文件夹下启动start-yarn.sh，才能正常启动ResourceManager以及NodeManager。 其中三条命令，要么选择start-dfs.sh加start-yarn.sh，要么选择start-all.sh，二选其一即可。 启动完成后，可以通过jps来查询各节点进程，正确的进程状态如下表 节点名 拥有进程 master NameNode、DataNode、Jps、NodeManager slave1 SecondaryNameNode、DataNode、Jps、NodeManager slave2 ResourceManager、DataNode、Jps、NodeManager 进程所在节点根据不同的集群规划会在不同的地方，但是以上进程必须拥有，如果没有对应进程，则证明启动失败，需要检查日志文件、报错信息进行配置文件的修改。 Hadoop伪分布式安装有关Hadoop的伪分布式配置，就简单的提一嘴，Hadoop的伪分布式是在只有一台机器时，模拟的集群配置，所以和完全分布式配置不同的地方在于，因为只有一台机器，所以所有内容必须搭建在同一台机器，即所有节点端口，都配置在master节点上（看你仅存的节点名叫什么hhh🤣）即可。 Hadoop on Yarn(HA)写在最前：关于HadoopHA的内容，个人理解不深，目前只能达到能做题目的水平，对配置项内的参数也并不是十分理解。此外，还需要注意的是，关于HadoopHA的后续安装，目前实测只成功了kafka、hive，失败报错flume的数据采集部分（仍未解决）😭😭。 Hadoop 的HA有两个前置环境需要安装，第一个是JAVA环境，第二个是Zookeeper环境。 JAVA环境在第一章中我们已经进行过了配置，这里就不再赘述。接下来我们来配置i所需要的第二种环境—— Zookeeper配置解压文件 1tar -zxvf /opt/software/zookeeper.tar.gz -C /opt/module/ 解压压缩包到指定文件夹中。 @配置环境变量1234vi /etc/profileexport ZOOKEEPER_HOME=/opt/module/apache-zookeeperexport PATH=$PATH:$ZOOKEEPER_HOME/binsource /etc/profile @配置文件#myid123cd /opt/module/apache-zookeepermkdir dataecho 1 &gt; data/myid 对于zookeeper，首先是每个节点的myid值，这个myid值再进行文件拷贝后，需要在不同的节点写上不同的值，值必须和zoo.cfg文件中server.x中x的值相同，注意对应节点。我的配置下，master节点为1，slave1节点为2，slave2节点为3。 #zoo.cfg1234567cd /opt/module/apche-zookeeper/confcp zoo_sample.cfg zoo.cfgvi /opt/module/apache-zookeeper/zoo.cfgdataDir=/opt/module/apache-zookeeper/dataserver.1=master:2888:3888server.2=slave1:2888:3888server.3=slave2:2888:3888 首先进入到zookeeper的conf文件夹下，conf文件夹下有一个zoo_sample.cfg文件，我们需要对这个进行修改，但是同样的，这个文件也只是一个模板文件，所以在进行修改配置之前，我们要先拷贝此文件。 zoo.cfg文件的修改内容为两处—— dataDir，这条属性存储的是zookeeper各个节点的myid值，路径设置为zookeeper根目录下的data文件夹，也就是指向myid值即可 server.x&#x3D;ip:2888:3888，这条属性就是对应myid的值，x即你在当前节点下myid所需要配置的值，ip可以是IP名也可以是IP地址，具体是什么，可以具体情况都试试看（🤣），因为曾经出现过IP名报错，但是IP地址可以使用的情况，但是hhh网上的都说不可以用IP地址，唔，具体原因还没有研究过，等后面再研究了看。 @启动集群123cd /opt/module/apache-zookeeper/binzkServer.sh startzkServer.sh status zookeeper的启动是有说法的，这个原理关系到zookeeper的选举机制。这里就先不进行赘述了。主要分为两个进程——Mode：follower和Mode：Leader。 同时还有一个需要注意的地方，zookeeper的启动需要三台同时启动，这个可以借助Xshell工具或者是自主编写脚本，或者另一种方式是单节点的一台一台启动。其中启动实践应该是控制在30000ms以内（还没找到具体的时间要求，但是很久以前好像在哪里看到过）。根据zookeeper的选举机制，第一台和第三台启动的节点为follow节点，第二台启动的节点为leader节点。 可以通过 查询状态 1zkServer.sh status 启动完成后jps查询 节点名称 进程状态 master Mode：follower slave1 Mode：leader slave2 Mode：follower zookeeper正常启动后进程状态如上表所示，注意：jps查询不会有结果 HadoopHA部署解压文件 1tar -zxvf /opt/software/hadoop-3.1.3 -C /opt/module/ @配置环境变量这里配置的版本是hadoop-3.1.3，所以环境变量中多出来的七项，在hadoop-2.7版本中不需要配置 123456789101112vi /etc/profileexport HADOOP_HOME=/opt/module/hadoop-3.1.3export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/bin//以下是hadoop-3.1.3版本中需要额外配置的，这些配置内容还可以配置在启动文件或者是hadoop-env.sh中export HDFS_NAMENODE_USER=rootexport HDFS_DATANODE_USER=rootexport HDFS_SECONDARYNAMENODE_USER=rootexport YARN_RESOURCEMANAGER_USER=rootexport YARN_NODEMANAGER_USER=root//以下是hadoop-3.1.3版本中HA所需要的新配置内容export HDFS_ZKFC_USER=rootexport HDFS_JOURNALNODE_USER=root 以上内容在启动时，如果没有配置或是配置有误，会error报错提示，但是，这里推荐配置好，不要到时候再进行修改，可能会出现解决不了报错问题。 @配置文件#hadoop-env.sh12vi /opt/module/hadoop-3.1.3/etc/hadoop/hadoop-env.shexport JAVA_HOME=/opt/module/jdk #core-site.xml12345678910111213141516171819202122232425262728293031323334vi /opt/module/hadoop-3.1.3/etc/hadoop/&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/dfs/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master:2181,slave1:2181,slave2:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- 👇❌ --&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;131072&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.users&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; fs.defaultFS：Hadoop FS 客户端使用的默认路径 hadoop.tmp.dir：Hadoop临时文件路径 ha.zookeeper.quorum：配置zk集群 dfs.journalnode.edits.dir：JournalNode守护进程将存储其本地状态的路径 io.file.buffer.size：SequenceFiles中使用的读&#x2F;写缓冲区的大小 hadoop.proxyuser.root.hosts：配置允许访问的主机 hadoop.proxyuser.root.groups：配置允许访问的用户组 hadoop.proxyuser.root.users：配置运行访问的用户 #hdfs-site.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172vi /opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;master:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;slave1:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;master:9870&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;slave1:9870&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://master:8485;slave1:8485;slave2:8485/mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/data/jn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluter&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence shell(/bin/bash) &lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;!-- 👇❌ --&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.connection-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-3.1.3/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.blocksize&lt;/name&gt; &lt;value&gt;268435456&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt; &lt;value&gt;100&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; dfs.nameservices：nameservices逻辑名称 dfs.ha.namenodes.mycluster：nameservices服务中每个NameNode的唯一标识符 dfs.ha.namenode.rpc-address.mycluster.nnX：nnX NameNode监听的完全限定的RPC地址 dfs.ha.namenode.http-address.mycluster.nnX：nnX NameNode监听的完全限定的HTTP地址 dfs.namenode.shared.edits.dir：标识NameNode 将在其中写入&#x2F;读取编辑的 JN 组的URL dfs.client.failover.proxy.provider.mycluster：HDFS客户端用来联系Active NameNode的Java类 dfs.ha.fencing.methods：一个脚本或JAVA类的列表，将在故障转移期间用于隔离Active NameNode dfs.ha.fencing.ssh.private-key-files：SSH 到 Active NameNode 并终止进程 dfs.ha.fencing.ssh.connect-timeout：SSH连接超时时长(毫秒) dfs.ha.automatic-failover.enabled：开启自动故障转移 dfs.replication：Hadoop的副本数量，默认为3 dfs.namenode.name.dir：在本地文件系统所在的NameNode的存储空间和持续化处理日志 dfs.datanode.data.dir：在本地文件系统所在的DataNode的存储空间和持续化处理日志 dfs.blocksize：用于大型文件系统的 HDFS 块大小为 256MB dfs.namenode.handler.count：NameNode 服务器线程来处理来自大量DataNode 的 RPC #yarn-site.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566vi /opt/module/hadoop-3.1.3/etc/hadoop/yarn-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;RMcluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;master&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;slave1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.zk.address&lt;/name&gt; &lt;value&gt;master:2181,slave1:2181,slave2:2181&lt;/value&gt; &lt;/property&gt; &lt;!-- 👇❌ --&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt; &lt;value&gt;5&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;master:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;slave1:8088&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; yarn.scheduler.minimum-allocation-mb：在资源管理器中分配给每个容器请求的最小内存限制。以 MB 为单位 yarn.scheduler.maximum-allocation-mb：在资源管理器中分配给每个容器请求的最大内存限制。以 MB 为单位 yarn.nodemanager.resource.cpu-vcores：可以为容器分配的 vcore 数量，这不用于限制 YARN 容器使用的物理内核数。默认为8 yarn.resourcemanager.ha.enabled：启用 RM HA yarn.resourcemanager.cluster-id：标识集群。选举人使用它来确保 RM 不会接管另一个集群的活动 yarn.resourcemanager.ha.rm-ids：RM 的逻辑 ID 列表 yarn.resourcemanager.hostname.rmX：指定 RM逻辑id,rmX对应的主机名 yarn.resourcemanager.webapp.address.rmX：指定 RM逻辑id,rmX对应的web地址 hadoop.zk.address：指定hadoop集群 #mapred-site.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354vi /opt/module/hadoop-3.1.3/etc/hadoop/mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;!-- 👇❌ --&gt; &lt;property&gt; &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=/opt/module/hadoop-3.1.3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=/opt/module/hadoop-3.1.3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.env&lt;/name&gt; &lt;value&gt;HADOOP_MAPRED_HOME=/opt/module/hadoop-3.1.3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt; &lt;value&gt;2048&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt; &lt;value&gt;-Xmx1536M&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt; &lt;value&gt;4096&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt; &lt;value&gt;-Xmx2560M&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.intermediate-done-dir&lt;/name&gt; &lt;value&gt;/mr-history/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.done-dir&lt;/name&gt; &lt;value&gt;/mr-history/done&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; mapreduce.framework.name：执行框架设置为 Hadoop YARN mapreduce.map.memory.mb：mapreduce.map.memory.mb mapreduce.map.java.opts：map任务子jvm的堆大小 mapreduce.reduce.memory.mb：reduce任务的资源限制 mapreduce.map.java.opts：reduce任务子jvm的堆大小 mapreduce.jobhistory.address：MapReduce JobHistory 服务器主机：端口 mapreduce.jobhistory.webapp.address：MapReduce JobHistory Server Web UI主机：端口 mapreduce.jobhistory.intermediate-done-dir：MapReduce 作业写入历史文件的目录 mapreduce.jobhistory.done-dir：历史文件由 MR JobHistory Server 管理的目录 #workers1234vi /opt/module/hadoop-3.1.3/etc/hadoop/workersmasterslave1slave2 @启动集群 分发节点 启动JournalNode（3台节点） 1hdfs --daemon start journalnode 格式化namenode 1hdfs namenode -format 格式化zkfc 1hdfs zkfc -formatZK 启动集群 1start-all.sh 备用NameNode复制元数据目录（2台分节点） 12hdfs namenode -bootstrapStandbyhdfs --daemon start namenode 查看所有NameNode状态 1hdfs haadmin -getAllServiceState 运行Pi程序测试 1yarn jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar pi 10 10 重启master namenode 12hdfs --daemon start namenodehdfs haadmin -getServiceState nn1","categories":["Hadoop"]},{"title":"Flume搭建","path":"/2024/04/25/flume-build/","content":"Flume配置解压文件 1tar -zxvf /opt/software/apache-flume -C /opt/module/ @配置环境123vi /etc/profileexport FLUME_HOME=/opt/module/apache-flumeexport PATH=$PATH:$FLUME_HOME/bin @配置文件#flume-env.sh12vi /opt/module/flume/conf/flume-env.shexport JAVA_HOME=/opt/module/jdk flume-env.sh是temlate模板文件，需要自己进行复制 #log4j.properties123vi /opt/module/flume/conf/log4j.propertiesflume.log.dir=/opt/module/flume/logsflume.log.file=flume.log @架包#guava.jar1cp /opt/module/hadoop/share/hadoop/common/lib/guava2.7.jar /opt/module/apache-flume/lib 和hive一样，guava.jar是hadoop3.1.3新版本中的问题，是apache项目中对应的版本无法和hadoop版本对应导致的问题，所以当前版本报错，需要把hadoop下的2.x版本的架包移动到对应项目的lib文件夹下，并且删除以前的旧版本，即可完成。 删除guava1.x.jar 1rm -rf /opt/module/apache-flume/lib/guava.1.x.jar 删除老版本架包再运行。 @数据采集flume的数据采集，可以在flume根目录下新建一个文件夹来保存数据采集的文件，创建flume采集文件需要用.conf做后缀名 #创建12mkdir -p /opt/module/apache-flume/job/a1.confvi /opt/module/apache-flume/job/a1.conf #文件配置方法一：exec类型1234567891011121314151617181920212223#使用s1、k1、c1代表source、sink、channela1.sources = s1a1.sinks = k1a1.channels = c1#descirbe/configuration the sourcea1.source.s1.type = exec#采集数据的位置，题目要求一般是namenode或是datanode，如果使用或，则两个都可a1.source.s1.command = tail -F /opt/module/hadoop/logs/hadoop-root-namenode-master.log#describe the sinka1.sinks.k1.type = hdfs#采集后存储的位置a1.sinks.k1.hdfs.path = hdfs://master:9000/flume1#use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100#bind the source and sink to the channela1.sources.s1.channels = c1a1.sinks.k1.channel = c1 方法二：TAILDIR类型1234567891011121314151617q1.sources = s1q1.sinks = k1q1.channels = c1 q1.sources.s1.type = TAILDIRq1.sources.s1.filegroups = f1q1.sources.s1.filegroups.f1 = /opt/module/hadoop/logs/.*log q1.sinks.k1.type = hdfsq1.sinks.k1.hdfs.path = hdfs://master:9000/flume2 q1.channels.c1.type = memoryq1.channels.c1.capacity = 1000q1.channels.c1.transactionCapacity = 100 q1.sources.s1.channels = c1q1.sinks.k1.channel = c1 根据测试，sinks.path后面的地址要和core-site.xml文件中的地址一样，后面的文件名自定义即可 根据测试，根目录下不创建表也可以进行数据采集。 #启动采集首先启动hdfs和yarn 启动采集文件 1bin/flume-ng agent --conf conf/ --name a1 --conf-file job/netcat-flume-logger.conf -Dflume.root.logger=INFO,console -Dflume.root.logger&#x3D;INFO,console表示输出采集过程； –name后面为agent名字，即配置文件中最前面的a1； –conf后面为flume&#x2F;conf的路径地址 –conf-file后面为项目文件地址 #查看文件查看采集的数据 1hdfs dfs -ls -R /flume1 创建表文件 1hadoop fs -mkdir -p /flume 根据测试，根目录下不创建表也可以进行数据采集。写在这里以防外一需要。","categories":["Flume"]},{"title":"Flink搭建","path":"/2024/04/25/flink-build/","content":"Flink配置解压文件 1tar -zxvf /opt/software/flink -C /opt/module @配置环境1234567vi /etc/profileexport FLINK_HOME=/opt/module/flinkexport PATH=$PATH:$FLINK_HOME/bin//以下配置是flink on yarn所需要的配置内容export HADOOP_CONF_DIR=/opt/module/hadoop/etc/hadoopexport HADOOP_CLASSPATH=`/opt/module/hadoop/bin/hadoop classpath`source /etc/profile @配置文件#flink-conf.yaml12vi /opt/module/flink/conf/flink-conf.yamljobmanager.rpc.address:master #masters12vi /opt/module/flink/conf/mastersmaster:8081 #workers123vi /opt/module/flink/conf/workersslave1slave2 @分发节点和启动集群#分发节点12345scp -r /opt/module/flume slave1:/opt/modulescp -r /opt/module/flume slave2:/opt/modulescp -r /etc/profile slave1:/etcscp -r /etc/profile slave2:/etcsource /etc/profile #启动集群启动集群 1start-cluster.sh 关闭集群 1stop-cluster.sh 启动resourcemanager 1yarn-daemon.sh start resourcemanager 启动nodemanager 1yarn-daemon.sh start nodemanager 执行测试代码 1flink run -m yarn-cluster -p 2 -yjm 2G -ytm 2G $FLINK_HOME/examples/batch/WordCount.jar @部分报错 报错 出错原因： 类加载器的相关报错，可能时类加载器的问题 解决方法： 可以使用配置”classloader.check-leaked-classloader“禁用此检查 编辑flink-conf.yaml 1classloader.check-leaked-classloader: false 需要注意的是，false前需要有一个空格键占位符，否则无法生效","categories":["Flink"]},{"title":"Clickhouse搭建","path":"/2024/04/25/clickhouse-build/","content":"ClickHouse搭建Clickhouse搭建简述clickhouse在进行搭建时，是使用clickhouse脚本进行搭建，所以解压clickhouse文件后直接运行文件的脚本即可 操作步骤解压文件1234tar -zxvf /opt/software/clickhouse-common-static-dbg-21.9.4.3/ -C /opt/module/tar -zxvf /opt/software/clickhouse-common-static-21.9.4.35.tgz -C /opt/module/tar -zxvf /opt/software/clickhouse-server-21.9.4.35.tgz -C /opt/module/tar -zxvf /opt/software/clickhouse-client-21.9.4.35.tgz -C /opt/module/ clickhouse有以上四个文件需要解压，解压无顺序要求 执行脚本1234./clickhouse-common-static-dbg-21.9.4.3//install/doinst.sh ./clickhouse-common-static-21.9.4.35/install/doinst.sh ./clickhouse-client-21.9.4.35/install/doinst.sh./clickhouse-server-21.9.4.35/install/doinst.sh 四个脚本文件严格意义上来说没有顺序，一般来说是先执行两个common文件，其次是client客户端文件，最后是server服务文件。 这样的顺序主要是方便截图。 common与client文件执行后没有结果显示，server文件执行后有结果显示，所以最后使用server文件 启动Clickhouse启动 1systemctl start clickhouse-server 重启 1systemctl restart clickhouse-server 查询状态 1systemctl status clickhouse-server 运行状态： 运行状态示例 关闭状态： 关闭状态示例 关闭 1systemctl stop clickhouse-server 登录Clickhouse1clickhouse-client --port 9000 --host master --password 123456 clickhouse的默认端口号为9000，但是hadoop3.x版本中的hadoop默认端口也是9000，所以会出现端口占用的情况，所以我们需要更改clickhouse的默认端口号以此来保证端口不会出现重复占用的情况。 更改Clickhouse端口号搜索test_shard_localhost，其中有8个端口为9000的文件需要修改为9001； 注意9440端口文件，即test_shard_localhost_secure不需要进行修改； 以下是修改文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107&gt; 590 &lt;remote_servers&gt;&gt; 591 &lt;!-- Test only shard config for testing distributed storage --&gt;&gt; 592 &lt;test_shard_localhost&gt;&gt; 593 &lt;!-- Inter-server per-cluster secret for Distributed queries&gt; 594 default: no secret (no authentication will be performed)&gt; 595 &gt; 596 If set, then Distributed queries will be validated on shards, so at least:&gt; 597 - such cluster should exist on the shard,&gt; 598 - such cluster should have the same secret.&gt; 599 &gt; 600 And also (and which is more important), the initial_user will&gt; 601 be used as current user for the query.&gt; 602 &gt; 603 Right now the protocol is pretty simple and it only takes into account:&gt; 604 - cluster name&gt; 605 - query&gt; 606 &gt; 607 Also it will be nice if the following will be implemented:&gt; 608 - source hostname (see interserver_http_host), but then it will depends from DNS,&gt; 609 it can use IP address instead, but then the you need to get correct on the initiator node.&gt; 610 - target hostname / ip address (same notes as for source hostname)&gt; 611 - time-based security tokens&gt; 612 --&gt;&gt; 613 &lt;!-- &lt;secret&gt;&lt;/secret&gt; --&gt;&gt; 614 &gt; 615 &lt;shard&gt;&gt; 616 &lt;!-- Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas). --&gt;&gt; 617 &lt;!-- &lt;internal_replication&gt;false&lt;/internal_replication&gt; --&gt;&gt; 618 &lt;!-- Optional. Shard weight when writing data. Default: 1. --&gt;&gt; 619 &lt;!-- &lt;weight&gt;1&lt;/weight&gt; --&gt;&gt; 620 &lt;replica&gt;&gt; 621 &lt;host&gt;localhost&lt;/host&gt;&gt; 622 &lt;port&gt;9001&lt;/port&gt;&gt; 623 &lt;!-- Optional. Priority of the replica for load_balancing. Default: 1 (less value has more priority). --&gt;&gt; 624 &lt;!-- &lt;priority&gt;1&lt;/priority&gt; --&gt;&gt; 625 &lt;/replica&gt;&gt; 626 &lt;/shard&gt;&gt; 627 &lt;/test_shard_localhost&gt;&gt; 628 &lt;test_cluster_two_shards_localhost&gt;&gt; 629 &lt;shard&gt;&gt; 630 &lt;replica&gt;&gt; 631 &lt;host&gt;localhost&lt;/host&gt;&gt; 632 &lt;port&gt;9001&lt;/port&gt;&gt; 633 &lt;/replica&gt;&gt; 634 &lt;/shard&gt;&gt; 635 &lt;shard&gt;&gt; 636 &lt;replica&gt;&gt; 637 &lt;host&gt;localhost&lt;/host&gt;&gt; 638 &lt;port&gt;9001&lt;/port&gt;&gt; 639 &lt;/replica&gt;&gt; 640 &lt;/shard&gt;&gt; 641 &lt;/test_cluster_two_shards_localhost&gt;&gt; 642 &lt;test_cluster_two_shards&gt;&gt; 643 &lt;shard&gt;&gt; 644 &lt;replica&gt;&gt; 645 &lt;host&gt;127.0.0.1&lt;/host&gt;&gt; 646 &lt;port&gt;9001&lt;/port&gt;&gt; 647 &lt;/replica&gt;&gt; 648 &lt;/shard&gt;&gt; 649 &lt;shard&gt;&gt; 650 &lt;replica&gt;&gt; 651 &lt;host&gt;127.0.0.2&lt;/host&gt;&gt; 652 &lt;port&gt;9001&lt;/port&gt;&gt; 653 &lt;/replica&gt;&gt; 654 &lt;/shard&gt;&gt; 655 &lt;/test_cluster_two_shards&gt;&gt; 656 &lt;test_cluster_two_shards_internal_replication&gt;&gt; 657 &lt;shard&gt;&gt; 658 &lt;internal_replication&gt;true&lt;/internal_replication&gt;&gt; 659 &lt;replica&gt;&gt; 660 &lt;host&gt;127.0.0.1&lt;/host&gt;&gt; 661 &lt;port&gt;9001&lt;/port&gt;&gt; 662 &lt;/replica&gt;&gt; 663 &lt;/shard&gt;&gt; 664 &lt;shard&gt;&gt; 665 &lt;internal_replication&gt;true&lt;/internal_replication&gt;&gt; 666 &lt;replica&gt;&gt; 667 &lt;host&gt;127.0.0.2&lt;/host&gt;&gt; 668 &lt;port&gt;9001&lt;/port&gt;&gt; 669 &lt;/replica&gt;&gt; 670 &lt;/shard&gt;&gt; 671 &lt;/test_cluster_two_shards_internal_replication&gt;&gt; 672 &lt;test_shard_localhost_secure&gt;&gt; 673 &lt;shard&gt;&gt; 674 &lt;replica&gt;&gt; 675 &lt;host&gt;localhost&lt;/host&gt;&gt; 676 &lt;port&gt;9440&lt;/port&gt;&gt; 677 &lt;secure&gt;1&lt;/secure&gt;&gt; 678 &lt;/replica&gt;&gt; 679 &lt;/shard&gt;&gt; 680 &lt;/test_shard_localhost_secure&gt;&gt; 681 &lt;test_unavailable_shard&gt;&gt; 682 &lt;shard&gt;&gt; 683 &lt;replica&gt;&gt; 684 &lt;host&gt;localhost&lt;/host&gt;&gt; 685 &lt;port&gt;9001&lt;/port&gt;&gt; 686 &lt;/replica&gt;&gt; 687 &lt;/shard&gt;&gt; 688 &lt;shard&gt;&gt; 689 &lt;replica&gt;&gt; 690 &lt;host&gt;localhost&lt;/host&gt;&gt; 691 &lt;port&gt;1&lt;/port&gt;&gt; 692 &lt;/replica&gt;&gt; 693 &lt;/shard&gt;&gt; 694 &lt;/test_unavailable_shard&gt;&gt; 695 &lt;/remote_servers&gt; 需要注意的是，从网上搜索的修改clickhouse端口号是（82行，查找&lt;tcp_port&gt;即可找到）： 1&gt; 82 &lt;tcp_port&gt;9001&lt;/tcp_port&gt; 删除监听文件1rm -rf /etc/clickhouse-server/config.d/listen.xml 注意事项 监听文件listen.xml 在执行./clickhouse-server-21.9.4.35/install/doinst.sh 时，会需要输入clickhouse的登陆密码，如果此时不输入密码，就不会出现监听文件。","categories":["Clickhouse"]},{"title":"数据可视化（JavaScipt）","path":"/2024/04/25/数据可视化（JS）/","content":"一些关于自己学习的内容，希望能对你有帮助~ 💯赛前整理🥇数据可视化🕐前置配置 🚩vue.config.js 🤐关闭语法检测1lintOnSave:false 没有关闭语法检测会导致echarts引用时出现报错，所以首先需要配置的信息是关闭语法检测 😁跨域123456789101112devServer:&#123; proxy:&#123; &quot;/api&quot;:&#123; target:&quot;接口地址&quot;, ws:true, changeOrigin:true, pathRewrite;&#123; &quot;^/api&quot;:&quot;&quot; &#125; &#125; &#125;&#125; target：接口的地址，会给定数据接口。 ws：webstocket协议，双向通道，在传输速度上优于http协议。 changeOrigin：devServer中，proxy的changeOrigin是false：changeOrigin请求头中host仍然是浏览器发送过来的host；如果设置成true：发送请求头中host会设置成target。在vue-cli3中，默认changeOrigin的值是true,意味着host设置成target，这与cue-cli2不一致，vue-cli2这个默认值是false。 pathRewrite：重写策略，在target后会存在其他参数，以&#x2F;api去添加，但是添加到具体的接口中没有&#x2F;api这几个字符，所以需要添加一个空字符串在传入参数的时候代替&#x2F;api。 🚩全局配置 🤔数据请求post请求和get请求，两个请求方式的请求方法大致相同，在请求时，method参数内根据填入的参数不同来进行不同的请求。 post请求有请求体，在安全方面要高于get请求 1234axios:(&#123; method:&quot;get&quot;, url:&quot;/api/参数api&quot;&#125;) 使用axios进行数据请求，在请求前，可以利用postman工具进行数据接口的调用，检查无误后使用axios进行数据请求。get请求需要配置参数method和参数url。前者是请求类型，后者是api参数。 12345678axios:(&#123; method:&quot;post&quot;, url:&quot;/api/参数api&quot;, data:&#123; startTime: yyyy.MM.dd, endTime: yyyy.MM.dd &#125;&#125;) post请求相对于get请求多了一个body请求体的内容，该内容存放在data参数中，也就是需要额外加入的内容。 在postman中，需要根据题目来进行设置，一般body中，传入的参数类型为json类型，返回类型也为json类型。 🕑电商 🚩柱状图 😏数据处理 🤑消费额最高&#x2F;最低的省份&#x2F;地区 需求分析：字段选择消费额、省份&#x2F;地区。求消费额的最大和最小值的总和，单一累加处理，考虑使用Map对象的get和set方法来拿到消费额总额。累加后需要进行排序，map没有内置的排序方法，将map对象转为数组后进行排序处理，取前几位或后几位使用splice方法进行筛选。 累加处理： 1234567891011//累加处理 利用map对象的set和get方法var map1 = new Map()data.forEach((e) =&gt;&#123; //判断对象中是否含有省份字段，如果有，则进行累加，如果没有，则传入第一个参数 if(map1.has(e.proviceName))&#123; var tmp = map1.get(e.proviceName) + e.consumptionAmount map1.set(e.proviceName , tmp) &#125;else&#123; map1.set(e.proviceName , e.consumptionAmount) &#125;&#125;) 排序筛选处理： 12345678//对象转为数组map1 = Array.from(map1)map1.sort((a,b) =&gt;&#123; //降序，升序写法则相反，因为为二维数组，处理数据为第二位，所以返回下标为1 //sort()方法写法必须是return返回，否则不会进行处理 return b[1] - a[1]&#125;).splice(5)//继续使用splice()方法筛选出前五的数据 😐各个省份&#x2F;地区消费额的平均数 需求分析：选择字段省份&#x2F;地区、消费额。需要求平均的消费额，只需要使用map方法提取到对应消费额的每一条数据，然后将所有数据累加再除以数据条数，同时在这一步可以进行保留小数点的操作，即可算出此省份&#x2F;地区的平均消费额。 提取数据到数组中： 1234567891011121314var map1 = new Map()data.forEach((e) =&gt;&#123; if(map1.has(e.proviceName))&#123; //和累加类似，只是此时传入的map对象中，对应的value值是一个数组。 var tmp1 = [] tmp1 = map1.get(e.proviceName) tmp1.push(e.consumptionAmount) map1.set(e.proviceName , tmp1) &#125;else&#123; //这里如果第二位不用[]包裹，tmp1无法push任何数据 //如果这里不用[]包裹，则需要定义一个数组来接收[e.consumptionAount] map.set(e.proviceName , [e.consumptionAmount]) &#125;&#125;) 求平均数处理： 12345678910111213//定义一个数组来收集求平均值后的数据var compare = [] map1.forEach((v,k))&#123; var sum = 0 v.forEach((e) =&gt;&#123; //求总数 sum += e &#125;) //求平均数，同时保留两位小数 var avgNum = (sum / v.length).toFixed(2) compare.push(&#123;provice:k , avg:avgNum&#125;)&#125; forEach：forEach有三个参数，（value，key，item），默认传参为value。其中，value代表的是元素，key代表的是元素索引，即下标，item代表的是整个数组或者对象。当传一个参数是，代表的就是其本身；当传两个参数时，第一个参数代表其元素本身，第二个参数代表下标，当传参对象是一个对象时，需要注意的是，key对应key，value对应value，所以传参中第一个参数为值，第二个参数为键。 排序筛选处理： 1234compare.sort((a,b) =&gt;&#123; //根据avg字段进行排序 return b.avg - a.avg&#125;).splice(5) 🤨各个省份&#x2F;地区消费额的中位数 需求分析：选择字段省份&#x2F;地区、消费额。中位数需要用map提取到所有的消费额数据，然后再对数组进行二次处理，首先先对数组进行排序，然后再取到中位数。 提取数据到数组中： 1234567891011121314var map1 = new Map()data.forEach((e) =&gt;&#123; if(map1.has(e.proviceName))&#123; //和累加类似，只是此时传入的map对象中，对应的value值是一个数组。 var tmp1 = [] tmp1 = map1.get(e.proviceName) tmp1.push(e.consumptionAmount) map1.set(e.proviceName , tmp1) &#125;else&#123; //这里如果第二位不用[]包裹，tmp1无法push任何数据 //如果这里不用[]包裹，则需要定义一个数组来接收[e.consumptionAount] map1.set(e.proviceName , [e.consumptionAmount]) &#125;&#125;) 排序并求中位数： 12345678910111213141516171819202122//tmp1数组用来拿到前五省份的中位数var tmp1 = []map1.forEach((v,k) =&gt;&#123; //排序操作 v.sort((a,b) =&gt;&#123; return b - a &#125;) //求中位数 v.every(() =&gt;&#123; //如果是取余为偶数，则中位数为取商和商小一位的数值相加求平均值 if(v.length % 2 == 0)&#123; var tmp2 = ((v[v.length / 2] + v[v.length / 2 - 1]) / 2).toFixed(2) tmp1.push(&#123;province:k , mid:tmp2&#125;) return false &#125;else&#123; var tmp2 = v[Math.floor(v.length / 2).toFixed(2)] tmp1.push(&#123;province:k , mid:tmp2&#125;) //forEach遍历没有break和continue，可以使用every进行代替，return false返回false，等效于break return false &#125; &#125;)&#125;) break：forEach遍历方法中不支持break和continue，如果需要使用continue时，可以采用return false或return true代替，如果要使用break时，可以使用try catch或every或some代替。实测return false可以达到break要求。 排序筛选处理： 123tmp1.sort((a,b) =&gt;&#123; return b.mid - a.mid&#125;).splice(5) 😎全局样式 xAxis type：category &#x2F; value data：xdata &#x2F; ❌ axisLabel：rotate:50 &#x2F; fontSize:18 yAxis type：category &#x2F; value data：xdata &#x2F; ❌ scale：true series type：bar data：ydata label：show:true &#x2F; position:direction 🚩折线图 😂数据处理 😌每年上架商品数量变化 数据分析：字段选择年份、商品名等任意字段。相对于使用计数的方法，其实通过map对象中get方法统计出现次数，然后统计map对象中数组的长度，即可达到统计上架商品数量的变化。 统计各年份数据： 12345678910var map1 = new Map()data.forEach((e) =&gt;&#123; if(map1.has(e.year))&#123; var tmp1 = [] tmp1 = map1.get(e.year) tmp1.push(e.id) map1.set(e.year , tmp1) &#125;else map1.set(e.year , [e.id])&#125;) 统计长度： 123456var xdata = []var ydata = []map.forEach((v,k) =&gt;&#123; xdata.push(k) ydata.push(v.length)&#125;) 🤪全局样式 xAxis type：category data：xdata axisLabel：rotate:50 &#x2F; fontSize:18 yAxis type： value data：xdata scale：true series type：line data：ydata label：show:true &#x2F; position:direction 🚩折柱混合图 😝数据处理 🤯柱状图表示前五省份平均消费额，折线图表示对应地区的平均消费额 需求分析：字段选择省份、地区、消费额。本题难点为如何对应省份和地区，因为对应（主键）字段为省份字段，所以根据省份来进行筛选。map1:{省份 &#x3D;&gt; 省份总额}，map2:{地区 &#x3D;&gt; 地区总额}，map3:{地区 &#x3D;&gt; 地区内省份个数}。用map1对象进行遍历，目标值是拿到每一个省份的名字，再次对元数据进行遍历，如果省份id和元数据中的省份id相等时，那么对省份计数，同时提取出元数据中的地区名称。将地区放入map2和map3中进行对应计算平均消费额，根据省份计数和map1中的省份总销售额进行计算平均消费额。 map1： 12345678var map1 = new Map()tmp1.forEach((e) =&gt;&#123; if(map1.has(e.province))&#123; tmp1.set(e.province , tmp1.get(e.province) + e.finalTotalAmount) &#125;else&#123; tmp1.set(e.province , e.finalTotalAmount) &#125;&#125;) map2和map3： 12345678910var map1 = new Map()tmp1.forEach((e) =&gt;&#123; if(map1.has(e.regionName))&#123; tmp2.set(e.regionName , tmp2.get(e.regionName) + e.finalTotalAmount) tmp3.set(e.regionName , tmp3.get(e.regionName) + 1) &#125;else&#123; tmp2.set(e.regionName , e.finalTotalAmount) tmp3.set(e.regionName , Number(1)) &#125;&#125;) 对应省份和地区并求平均消费额： 1234567891011121314151617181920212223var res = []map1.for((v,k) =&gt;&#123; //第一步是提取需要的数据 //提取省份 var pro = k //定义计数器 var count = 0 //定义一个地区值用于接收“省份对应地区” var reg_compare //提取省份对应消费总额，计算需要使用 var pro_v = v //进入元数据开始判断 tmp1.forEach((e) =&gt;&#123; if(e.provinceName == pro)&#123; count += 1 reg_compare = e.regionName &#125; &#125;) //计算 var pro_avg = (pro_v / count).toFixed(2) var reg_avg = (map2.get(reg_compare) / map3.get(reg_compare)).toFixed(2) res.push(&#123;pro:pro , reg:reg_compare , pro_avg:pro_avg , reg_avg:reg_avg&#125;)&#125;) 排序筛选处理： 123res.sort((a,b) =&gt;&#123; return b.pro_avg - a.pro_avg&#125;).splice(5) 🥴全局样式 xAxis type：category data：xdata axisLabel：rotate:50 &#x2F; fontSize:18 yAxis type： value data：xdata scale：true legend data：[] &#x2F; ❌ series type：line ＆ bar data：ydata label：show:true &#x2F; position:direction name：[“图例”] 🚩饼状图 😜数据处理 😴饼状图展示各地区消费能力 需求分析：求各地区的消费能力，即以地区为主键字段，统计此地区所有的消费金额&#x2F; 统计： 12345678var map1 = new Map()tmp1.forEach((e) =&gt;&#123; if(map1.has(e.regionName))&#123; map1.set(e.regionName , map1.get(e.regionName) + e.finalTotalAmount) &#125;else&#123; map1.set(e.regionName , e.finalTotalAmount) &#125;&#125;) 🥱玫瑰图展示各地区平均消费能力 需求分析：求各地区的消费能力，即以地区为主键字段，统计此地区所有的消费金额的平均额 求各地区的消费数组： 1234567891011var map1 = new Map()tmp1.forEach((e) =&gt;&#123; if(map1.has(e.regionName))&#123; var tmp2 = [] tmp2 = map1.get(e.regionName) tmp2.push(e.finalTotalAmount) map1.set(e.regionName , tmp2) &#125;else&#123; map1.set(e.regionName , [e.finalTotalAmount]) &#125;&#125;) 求平均额： 123456789var res = []map1.forEach((v,k) =&gt;&#123; var sum = 0 v.forEach(() =&gt;&#123; sum += 1 &#125;) var avg = (sum / v.length).toFixed(2) res.push(&#123;value:k , name:avg&#125;)&#125;) 🤤全局样式 series data：[{value：xx，name：xx}] itemStyle：normal：{label：{show：true ， formatter：” {a} {b} {c} {d}%”}} roseType：radius &#x2F; area legend data：[] &#x2F; ❌ 🚩散点图 😤数据处理 👹散点图展示每年上架商品数量的变化 👺散点图展示省份平均消费额 😬全局样式 xAxis type：category &#x2F; value data：xdata axisLabel：rotate:50 &#x2F; fontSize:18 yAxis type： value &#x2F; category data：xdata scale：true series type：scatter label：show：true，position：direction 🕒工业 😮‍💨数据处理方法 🚩元数据筛选.filter() 12345var dataOrigin = arr.data.filter((e) =&gt; (i.ChangeStartTime.split(&quot;T&quot;)[0].split(&quot;-&quot;)[0]) == 2021 &amp;&amp; (i.ChangeStartTime.split(&quot;T&quot;)[0].split(&quot;-&quot;)[1]) == 10 &amp;&amp; (i.ChangeStartTime.split(&quot;T&quot;)[0].split(&quot;-&quot;)[2]) == 12) 筛选数据：在元数据中，ChangeStartTime字段，年份为2021年，月份为10月，日期为12日的所有数据。 filter：方法用于过滤元素，filter的用法和map相似，但是不同的是，filter会把传入的函数依次用于每一个元素，根据返回值是true或是false来进行判断是保留或是丢弃该元素。 🚩数据去重.map() 123456var deduplication = new Map()dataOrigin.forEach((e) =&gt;&#123; if(!deduplication.has(e.ChangeID))&#123; deduplication.set(e.ChangeID , true) &#125;&#125;) 通过.map()来进行数据的去重，定义一个map对象，遍历元数据，对map对象进行判断，如果不存在ChangID（去重依据），那么就将这一条数据存入map对象中，同时完成数据的记录，完成数据的去重。 🚩时间戳转换与计算 中国标准时间：Thu Feb 28 2019 17:11:43 GMT+0800 JS默认中国标准时间是 GMT时间.由于我们国家采用的是东八区时间,因此是GMT +0800 ISO8601标准时间：2019-02-28T09:51:45.540Z 其中T表示合并,Z表示UTC时间 时间戳是指格林威治时间1970年01月01日00时00分00秒(北京时间1970年01月01日08时00分00秒)起至现在的总秒数。java的date默认精度是毫秒，也就是说生成的时间戳就是13位的，而像c++或者php生成的时间戳默认就是10位的，因为其精度是秒。 12345dataOrigin.forEach((e) =&gt;&#123; var start = new Date(e.ChangeStartTime).getTime() var end = new Date(e.ChangeEndTime).getTime() var tim = parseFloat(end - start) / 1000&#125;) new Date()：获取时间，Date对象用于处理日期和时间; getTime()：方法返回从1970 年 1 月 1 日午夜到指定日期之间的毫秒数； parseFloat()：函数解析字符串并返回浮点数 描述： （图片找不到了额……等我修一下） 此函数确定指定字符串中的第一个字符是否为数字。如果是，它会解析字符串直到到达数字的末尾，并将数字作为数字而不是字符串返回。 注意：只返回字符串中的第一个数字！ 注释：允许前导和尾随空格。 注释：如果第一个字符不能转换为数字，parseFloat() 返回 NaN。 🚩数据key如何赋值为空在进行数据处理时，会出现不同的对象对应的值不一样。 比如求某个设备各个状态下的持续时长，可能就会出现某台设备没有某个状态，那么通过map对象set到的方法，就不会显示这一个状态的任何数据。 那么，通过提前存入map(key &#x3D;&gt; value)，就可以避免此问题，如果出现了对应数据，那么则会显示你存入的数据value。 12345678var run = new Map()var stand = new Map()var stop = new Map()tmp1.forEach((e) =&gt;&#123; run.set(e.machine , 0) stand.set(e.machine , 0) stop.set(e.machine , 0)&#125;) 通过set提前存入数据，map则会如以下形式，这样即使在后续处理中，某台设备没有此状态的任何数据，也不会忽略此条数据，而是存入了（key &#x3D;&gt; 0）此条数据，避免了后续手动插入数据的麻烦。 map( {machine1 &#x3D;&gt; 0}， {machine2 &#x3D;&gt; 0}， {machine3 &#x3D;&gt; 0}) 🚩dataset数据集使用情况目前来说，唯一推荐使用数据集的地方是多条数据、数据成表的格式数据。其他暂时不考虑使用dataset 123dataset:&#123; source:[data1,data2,data3]&#125; 数据集： （图片找不到了额……等我修一下） data1为数据头，数据头在生成图表后为图例，读取数据方式为从上到下； data2为数据集，数据集中的头部为图表横坐标； 如果不满足以上条件的图，建议直接绘制，采用data的方式去导入数据，而不采用数据集 🚩关于拼接比较拼接比较的目的是处理部分内容数据，思路是先在处理后数组中将需要保留的字段进行拼接，然后另一个新数组也同时获取这个比较字段，同时增加新字段，新字段就是我们需要处理的部分内容数据，此时相比较两个数组，根据数组的属性（如出现次数，是否存在）来对内容数据进行处理（如增删查改，累加，赋值）。 举个例子⑧🙋🌰： 每日各车间平均运行时长 此时，需要抽取的数据为日期信息，车间信息，运行状态信息，以及时长数据。需要处理的是时长，但是处理的方式只能是一对一，所以此时我们将日期信息和车间信息进行拼接来处理时长数据。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950let tempArr = []; for (let i in state.sumda) &#123; let list = state.sumda[i]; if (!(list.ChangeTime &amp;&amp; list.MachineFactory)) &#123; // 如果 20GP continue; &#125; else &#123; let compare = &#123; type: list.ChangeTime + &quot; &quot; + list.MachineFactory, &#125;; //作比较的对象 (因为我需要这种格式，你也可以自定义格式) if (tempArr.length == 0) &#123; //这段代码必须要写 let num = Number(list.times); let cou = Number(list.count); // console.log(num); // console.log(cou); let obj = &#123; type: list.ChangeTime + &quot; &quot; + list.MachineFactory, total: num, count: cou, &#125;; tempArr.push(obj); // console.log(tempArr,&quot;0000000&quot;); &#125; else &#123; let flag = true; for (let j in tempArr) &#123; //这个循环只做一个操作，有相同类型的就进行统计 if (tempArr[j][&quot;type&quot;] == compare.type) &#123; tempArr[j][&quot;total&quot;] += Number(list.times); tempArr[j][&quot;count&quot;] += Number(list.count); flag = false; //已经统计过的，就不要再重复了 break; &#125; &#125; if (flag) &#123; //如果没有这种类型的，就push进去 let num = Number(list.times); let cou = Number(list.count); var obj = &#123; type: list.ChangeTime + &quot; &quot; + list.MachineFactory, total: num, count: cou, &#125;; tempArr.push(obj); console.log(tempArr); &#125; &#125; &#125; &#125; console.log(tempArr); 🚩删除或添加指定元素.splice()","categories":["JavaScript"]},{"title":"Hudi搭建","path":"/2024/04/25/hudi-build/","content":"Hudi搭建Maven 解压文件1tar -zxvf /opt/software/apache-maven-3.6.1-bin.tar.gz -C /opt/module/ 配置环境变量1234vi /etc/profile#mavenexport MAVEN_HOME=/opt/module/apache-maven-3.6.1-binexport PATH=$PATH:$MAVEN_HOME/bin 添加阿里镜像1234567vi /opt/module/maven-3.6.1/conf/settings.xml&lt;mirror&gt; &lt;id&gt;nexus-aliyun&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;name&gt;Nexus aliyun&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;&lt;/mirror&gt; 修改本地仓库12vi /opt/module/maven-3.6.1/conf/settings.xml&lt;localRepository&gt;/opt/software/RepMaven&lt;/localRepository&gt; Hudi 解压文件1tar -zxvf /opt/software/hudi-0.11.0.src.tgz -C /opt/module/ 修改POM文件修改依赖组件版本123vim /opt/software/hudi-0.12.0/pom.xml&lt;hadoop.version&gt;3.1.3&lt;/hadoop.version&gt;&lt;hive.version&gt;3.1.2&lt;/hive.version&gt; 修改源码兼容hadoop3.x12vim /opt/software/hudi-0.12.0/hudi-common/src/main/java/org/apache/hudi/common/table/log/block/HoodieParquetDataBlock.javatry(FSDataOutputStream outputStream) = new FSDataOutputStream(baos) 在括号中加入null，修改后如下 1try(FSDataOutputStream outputStream) = new FSDataOutputStream(baos,null) 位置在文件110行左右，可以直接搜索baos进行操作 hudi-spark-bundle修改了Hive版本为3.1.2，其携带的jetty是0.9.3，hudi本身用的0.9.4，存在依赖冲突。修改hudi-spark-bundle的pom文件，排除低版本jetty，添加hudi指定版本的jetty。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118vim /opt/software/hudi-0.12.0/packaging/hudi-spark-bundle/pom.xml&lt;!-- Hive --&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;hive.groupid&#125;&lt;/groupId&gt; &lt;artifactId&gt;hive-service&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;scope&gt;$&#123;spark.bundle.hive.scope&#125;&lt;/scope&gt; //hive-service添加内容 &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.pentaho&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;hive.groupid&#125;&lt;/groupId&gt; &lt;artifactId&gt;hive-service-rpc&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;scope&gt;$&#123;spark.bundle.hive.scope&#125;&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;hive.groupid&#125;&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;scope&gt;$&#123;spark.bundle.hive.scope&#125;&lt;/scope&gt; //hive-jdbc添加内容 &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;hive.groupid&#125;&lt;/groupId&gt; &lt;artifactId&gt;hive-metastore&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;scope&gt;$&#123;spark.bundle.hive.scope&#125;&lt;/scope&gt; //hive-metastore添加内容 &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.datanucleus&lt;/groupId&gt; &lt;artifactId&gt;datanucleus-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;hive.groupid&#125;&lt;/groupId&gt; &lt;artifactId&gt;hive-common&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;scope&gt;$&#123;spark.bundle.hive.scope&#125;&lt;/scope&gt; //hive-common添加内容 &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.eclipse.jetty.orbit&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; &lt;!-- 增加hudi配置版本的jetty --&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-server&lt;/artifactId&gt; &lt;version&gt;$&#123;jetty.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-util&lt;/artifactId&gt; &lt;version&gt;$&#123;jetty.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-webapp&lt;/artifactId&gt; &lt;version&gt;$&#123;jetty.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-http&lt;/artifactId&gt; &lt;version&gt;$&#123;jetty.version&#125;&lt;/version&gt; &lt;/dependency&gt; hudi-utilities-bundle修改hudi-utilities-bundle的pom文件，排除低版本jetty，添加hudi指定版本的jetty 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149vim /opt/software/hudi-0.12.0/packaging/hudi-utilities-bundle/pom.xml &lt;!-- Hoodie --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hudi&lt;/groupId&gt; &lt;artifactId&gt;hudi-common&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; //hudi-common添加内容 &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hudi&lt;/groupId&gt; &lt;artifactId&gt;hudi-client-common&lt;/artifactId&gt; &lt;version&gt;$&#123;project.version&#125;&lt;/version&gt; //hudi-client-common添加内容 &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- Hive --&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;hive.groupid&#125;&lt;/groupId&gt; &lt;artifactId&gt;hive-service&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;scope&gt;$&#123;utilities.bundle.hive.scope&#125;&lt;/scope&gt; //hive-service添加内容 &lt;exclusions&gt; &lt;exclusion&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.pentaho&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;hive.groupid&#125;&lt;/groupId&gt; &lt;artifactId&gt;hive-service-rpc&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;scope&gt;$&#123;utilities.bundle.hive.scope&#125;&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;hive.groupid&#125;&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;scope&gt;$&#123;utilities.bundle.hive.scope&#125;&lt;/scope&gt; //hive-jdbc添加内容 &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;hive.groupid&#125;&lt;/groupId&gt; &lt;artifactId&gt;hive-metastore&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;scope&gt;$&#123;utilities.bundle.hive.scope&#125;&lt;/scope&gt; //hive-metastore添加内容 &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.datanucleus&lt;/groupId&gt; &lt;artifactId&gt;datanucleus-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;$&#123;hive.groupid&#125;&lt;/groupId&gt; &lt;artifactId&gt;hive-common&lt;/artifactId&gt; &lt;version&gt;$&#123;hive.version&#125;&lt;/version&gt; &lt;scope&gt;$&#123;utilities.bundle.hive.scope&#125;&lt;/scope&gt;、 //hive-common添加内容 &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.eclipse.jetty.orbit&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;*&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!-- 增加hudi配置版本的jetty --&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-server&lt;/artifactId&gt; &lt;version&gt;$&#123;jetty.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-util&lt;/artifactId&gt; &lt;version&gt;$&#123;jetty.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-webapp&lt;/artifactId&gt; &lt;version&gt;$&#123;jetty.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.eclipse.jetty&lt;/groupId&gt; &lt;artifactId&gt;jetty-http&lt;/artifactId&gt; &lt;version&gt;$&#123;jetty.version&#125;&lt;/version&gt; &lt;/dependency&gt; 执行编译命令 打包命令 12mvn clean package-DskipTests -Dspark3.1 -Dflink1.14 -Dscala-2.12 -Dhadoop.version=3.1.3 -Pflink-bundle-shade-hive3 -DskipTests，不执行测试用例，但编译测试用例类生成相应的class文件至target&#x2F;test-classes下。 -Dmaven.test.skip&#x3D;true，不执行测试用例，也不编译测试用例类。 -Dspark\t-Dflink\t-Dscala\t为对应组件的版本 -Pflink-bundle-shade-hive3\t如果不指定此条，会在hudi编译时对flink-bundle的架包报错 需要注意的是，hudi的编译命令需要在hudi的根目录下进行执行 运行案例移动jar包我们编译好的jar包存储在hudi根目录下的packaging中，我们需要进入packaging文件夹找到hudi-spark3.1-bundle_2.12-0.11.0.jar架包即可。 1cd /opt/module/hudi-0.11.0/packaging/hudi-spark-bundle/targe 随后移动架包到spark目录下的jars文件夹中 1cp /opt/module/hudi-0.11.0/packaging/hudi-spark-bundle/target/hudi-spark3.1-bundle_2.12-0.11.0 /opt/module/spark-3.1.1-bin-hadoop3.2/jars/ 进入spark-shell123spark-shell \\--conf &#x27;spark.serializer=org.apache.spark.serializer.KryoSerializer&#x27; \\--conf &#x27;spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension&#x27; \\ –conf ‘spark.serializer&#x3D;org.apache.spark.serializer.KryoSerializer’\t序列化 –conf ‘spark.sql.extensions&#x3D;org.apache.spark.sql.hudi.HoodieSparkSessionExtension’\t集成spark 运行案例1234567891011121314151617181920212223242526import org.apache.hudi.QuickstartUtils._import scala.collection.JavaConversions._import org.apache.spark.sql.SaveMode._import org.apache.hudi.DataSourceReadOptions._import org.apache.hudi.DataSourceWriteOptions._import org.apache.hudi.config.HoodieWriteConfig._import org.apache.hudi.common.model.HoodieRecord val tableName = &quot;hudi_trips_cow&quot;val basePath = &quot;file:///tmp/hudi_trips_cow&quot;val dataGen = new DataGenerator val inserts = convertToStringList(dataGen.generateInserts(10))val df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))df.write.format(&quot;hudi&quot;). options(getQuickstartWriteConfigs). option(PRECOMBINE_FIELD_OPT_KEY, &quot;ts&quot;). option(RECORDKEY_FIELD_OPT_KEY, &quot;uuid&quot;). option(PARTITIONPATH_FIELD_OPT_KEY, &quot;partitionpath&quot;). option(TABLE_NAME, tableName). mode(Overwrite). save(basePath) val tripsSnapshotDF = spark.read.format(&quot;hudi&quot;).load(basePath + &quot;/*/*/*/*&quot;)tripsSnapshotDF.createOrReplaceTempView(&quot;hudi_trips_snapshot&quot;)spark.sql(&quot;select fare, begin_lon, begin_lat, ts from hudi_trips_snapshot where fare &gt; 20.0&quot;).show() 结果 案例结果图","categories":["Hudi"]}]